# PgVector 学习详细步骤说明

本文档为每个学习阶段提供详细的操作步骤和代码示例框架。

---

## 📚 第一阶段：环境准备与基础认知

### 步骤 1.1：检查 Docker 环境

**目标**：确认 PostgreSQL 容器正在运行

**操作步骤**：
```bash
# 1. 检查 Docker 容器状态
docker ps | grep postgres

# 2. 查看容器详细信息
docker inspect <container_name>

# 3. 检查容器日志
docker logs <container_name>
```

**预期结果**：看到正在运行的 PostgreSQL 容器

---

### 步骤 1.2：连接数据库

**目标**：使用 psql 或其他客户端连接到数据库

**操作步骤**：
```bash
# 方法1：使用 docker exec 进入容器后连接
docker exec -it <container_name> psql -U <username> -d <database_name>

# 方法2：从宿主机连接（需要映射端口）
psql -h localhost -p 5432 -U <username> -d <database_name>

# 方法3：使用图形化工具（如 DBeaver、pgAdmin）
```

**验证**：能够成功连接到数据库并执行 SQL 命令

---

### 步骤 1.3：查看 PostgreSQL 版本

**目标**：确认 PostgreSQL 版本符合要求（建议 11+）

**SQL 命令**：
```sql
-- 查看 PostgreSQL 版本
SELECT version();

-- 查看当前数据库信息
SELECT current_database(), current_user;
```

**预期结果**：显示 PostgreSQL 版本信息，确认版本 >= 11.0

---

### 步骤 1.4：理解向量数据库基础

**学习要点**：
- **向量（Vector）**：一组数值的数组，用于表示数据的高维特征
- **嵌入（Embedding）**：将文本、图像等数据转换为向量的过程
- **相似度度量**：
  - 余弦相似度（Cosine Similarity）：适合文本相似度
  - 欧氏距离（Euclidean Distance）：适合空间距离
  - 内积（Inner Product）：适合某些机器学习模型

**思考问题**：
1. 为什么需要向量数据库？
2. PgVector 相比专业向量数据库有什么优势？
3. 什么场景适合使用向量检索？

---

## 🔧 第二阶段：PgVector 安装与配置

### 步骤 2.1：在 Docker 容器中安装 pgvector 扩展

**目标**：安装 pgvector 扩展

**操作步骤**：

**方法1：如果 Docker 镜像已包含 pgvector**
```sql
-- 直接启用扩展
CREATE EXTENSION IF NOT EXISTS vector;
```

**方法2：如果 Docker 镜像未包含，需要手动安装**
```bash
# 进入容器
docker exec -it <container_name> bash

# 在容器中安装 pgvector（需要 root 权限）
# 具体安装方法取决于你的 Docker 镜像基础
# 例如使用 pgvector/pgvector 官方镜像：
# docker pull pgvector/pgvector:pg15
```

**验证**：
```sql
-- 检查扩展是否已安装
SELECT * FROM pg_extension WHERE extname = 'vector';

-- 查看扩展版本
SELECT extversion FROM pg_extension WHERE extname = 'vector';
```

---

### 步骤 2.2：创建测试数据库

**目标**：创建一个专门用于学习的测试数据库

**SQL 命令**：
```sql
-- 创建数据库
CREATE DATABASE pgvector_learning;

-- 连接到新数据库
\c pgvector_learning

-- 在新数据库中启用 pgvector 扩展
CREATE EXTENSION IF NOT EXISTS vector;
```

---

### 步骤 2.3：理解 vector 数据类型

**目标**：了解如何创建包含向量列的表

**数据类型说明**：
- `vector(n)`：固定维度 n 的向量
- `vector`：可变维度向量（默认最大 16,000 维）

**SQL 示例**：
```sql
-- 创建包含固定维度向量的表
CREATE TABLE items (
    id SERIAL PRIMARY KEY,
    name TEXT,
    embedding vector(128)  -- 128 维向量
);

-- 创建包含可变维度向量的表
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    content TEXT,
    embedding vector  -- 可变维度
);
```

**实践任务**：
```sql
-- 1. 创建一个测试表 items，包含 128 维向量
-- 2. 插入一些测试数据
INSERT INTO items (name, embedding) 
VALUES 
    ('item1', '[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128]'::vector);

-- 3. 查询验证数据
SELECT id, name, embedding FROM items;
```

---

### 步骤 2.4：基础表结构设计

**设计原则**：
1. **主键**：建议使用自增 ID 或 UUID
2. **元数据字段**：存储与向量相关的其他信息（如文本、标签、时间戳等）
3. **向量字段**：使用 vector 类型存储嵌入向量

**示例表结构**：
```sql
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    name TEXT NOT NULL,
    description TEXT,
    category TEXT,
    price DECIMAL(10, 2),
    embedding vector(384),  -- 384 维向量（例如 sentence-transformers）
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 添加索引（后续学习）
CREATE INDEX ON products USING ivfflat (embedding vector_cosine_ops);
```

---

## 🚀 第三阶段：基础操作实践

### 步骤 3.1：向量数据插入

**目标**：掌握如何插入向量数据

**SQL 示例**：
```sql
-- 单条插入
INSERT INTO items (name, embedding) 
VALUES ('item1', '[0.1,0.2,0.3,0.4,0.5]'::vector);

-- 批量插入
INSERT INTO items (name, embedding) VALUES
    ('item2', '[0.2,0.3,0.4,0.5,0.6]'::vector),
    ('item3', '[0.3,0.4,0.5,0.6,0.7]'::vector),
    ('item4', '[0.4,0.5,0.6,0.7,0.8]'::vector);

-- 从数组生成向量（使用 Python 或函数）
-- 假设有数组 ARRAY[0.1,0.2,0.3,0.4,0.5]
```

**Python 辅助脚本示例**（后续阶段使用）：
```python
import psycopg2
import numpy as np

# 生成随机向量
vector = np.random.rand(128).tolist()
vector_str = '[' + ','.join(map(str, vector)) + ']'

# 插入数据库
conn = psycopg2.connect(...)
cur = conn.cursor()
cur.execute("INSERT INTO items (name, embedding) VALUES (%s, %s::vector)", 
            ('item_name', vector_str))
conn.commit()
```

---

### 步骤 3.2：余弦相似度搜索

**目标**：使用余弦相似度找到最相似的向量

**核心运算符**：`<=>` （余弦距离，越小越相似）

**SQL 示例**：
```sql
-- 找到与查询向量最相似的 10 条记录
SELECT 
    id,
    name,
    1 - (embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector) AS similarity
FROM items
ORDER BY embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;

-- 使用变量（在函数或应用中）
-- 假设查询向量存储在变量中
WITH query_vec AS (
    SELECT '[0.1,0.2,0.3,0.4,0.5]'::vector AS vec
)
SELECT 
    i.id,
    i.name,
    1 - (i.embedding <=> q.vec) AS similarity
FROM items i, query_vec q
ORDER BY i.embedding <=> q.vec
LIMIT 10;
```

**注意**：
- `<=>` 返回距离（0 表示完全相同，1 表示完全不相似）
- `1 - distance` 得到相似度（1 表示完全相同，0 表示完全不相似）
- 余弦距离越小，相似度越高

---

### 步骤 3.3：欧氏距离搜索

**目标**：使用欧氏距离进行相似度搜索

**核心运算符**：`<->` （欧氏距离，越小越相似）

**SQL 示例**：
```sql
-- 使用欧氏距离找到最相似的记录
SELECT 
    id,
    name,
    embedding <-> '[0.1,0.2,0.3,0.4,0.5]'::vector AS distance
FROM items
ORDER BY embedding <-> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;
```

**适用场景**：
- 适合表示空间距离的场景
- 某些机器学习模型使用欧氏距离

---

### 步骤 3.4：负内积搜索

**目标**：使用负内积进行相似度搜索

**核心运算符**：`<#>` （负内积，越大越相似）

**SQL 示例**：
```sql
-- 使用负内积找到最相似的记录
SELECT 
    id,
    name,
    embedding <#> '[0.1,0.2,0.3,0.4,0.5]'::vector AS negative_inner_product
FROM items
ORDER BY embedding <#> '[0.1,0.2,0.3,0.4,0.5]'::vector DESC
LIMIT 10;
```

**注意**：负内积越大表示越相似，所以使用 `DESC` 排序

---

### 步骤 3.5：带阈值的相似度搜索

**目标**：只返回相似度超过阈值的记录

**SQL 示例**：
```sql
-- 余弦相似度阈值搜索（相似度 > 0.8）
SELECT 
    id,
    name,
    1 - (embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector) AS similarity
FROM items
WHERE 1 - (embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector) > 0.8
ORDER BY embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;

-- 欧氏距离阈值搜索（距离 < 0.5）
SELECT 
    id,
    name,
    embedding <-> '[0.1,0.2,0.3,0.4,0.5]'::vector AS distance
FROM items
WHERE embedding <-> '[0.1,0.2,0.3,0.4,0.5]'::vector < 0.5
ORDER BY embedding <-> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;
```

---

### 步骤 3.6：结合元数据的混合搜索

**目标**：在相似度搜索的基础上，结合其他条件过滤

**SQL 示例**：
```sql
-- 在特定类别中搜索相似产品
SELECT 
    id,
    name,
    category,
    1 - (embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector) AS similarity
FROM products
WHERE category = 'electronics'
ORDER BY embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;

-- 结合价格范围的相似度搜索
SELECT 
    id,
    name,
    price,
    1 - (embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector) AS similarity
FROM products
WHERE price BETWEEN 100 AND 500
ORDER BY embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;
```

**实践任务**：
1. 创建包含至少 100 条向量数据的表
2. 实现余弦相似度 Top-10 搜索
3. 实现欧氏距离 Top-10 搜索
4. 实现带相似度阈值的过滤搜索
5. 实现结合元数据的混合搜索

---

## ⚡ 第四阶段：索引优化学习

### 步骤 4.1：理解索引的重要性

**问题**：不使用索引时，向量搜索需要扫描所有向量，计算与查询向量的距离，时间复杂度为 O(n)

**解决方案**：使用向量索引可以大幅提升搜索速度，尤其是数据量大的时候

**索引类型**：
1. **HNSW**：适合高维向量、频繁查询
2. **IVFFlat**：适合大规模数据、批量查询

---

### 步骤 4.2：创建 HNSW 索引

**目标**：创建 HNSW 索引并测试性能

**HNSW 参数说明**：
- `m`：每个节点连接的最大邻居数（默认 16，建议 16-64）
- `ef_construction`：构建索引时搜索的候选数（默认 64，建议 64-200）

**SQL 示例**：
```sql
-- 创建 HNSW 索引（余弦距离）
CREATE INDEX ON items 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- 创建 HNSW 索引（欧氏距离）
CREATE INDEX ON items 
USING hnsw (embedding vector_l2_ops)
WITH (m = 16, ef_construction = 64);

-- 创建 HNSW 索引（内积）
CREATE INDEX ON items 
USING hnsw (embedding vector_ip_ops)
WITH (m = 16, ef_construction = 64);
```

**性能测试**：
```sql
-- 测试查询时间（使用 EXPLAIN ANALYZE）
EXPLAIN ANALYZE
SELECT id, name
FROM items
ORDER BY embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;
```

**调整 ef_search**：
```sql
-- 调整查询时的搜索参数（在查询前设置）
SET hnsw.ef_search = 100;  -- 默认 40，增大可以提升准确度但降低速度

-- 执行查询
SELECT id, name
FROM items
ORDER BY embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;
```

---

### 步骤 4.3：创建 IVFFlat 索引

**目标**：创建 IVFFlat 索引并测试性能

**IVFFlat 参数说明**：
- `lists`：聚类中心数量（默认 rows/1000，建议 rows/1000 到 rows/10）
- 需要足够的数据才能有效（建议至少 1000 条记录）

**SQL 示例**：
```sql
-- 首先确保有足够的数据（至少 1000 条）
-- 创建 IVFFlat 索引（余弦距离）
CREATE INDEX ON items 
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- 创建 IVFFlat 索引（欧氏距离）
CREATE INDEX ON items 
USING ivfflat (embedding vector_l2_ops)
WITH (lists = 100);
```

**性能测试**：
```sql
-- 测试查询时间
EXPLAIN ANALYZE
SELECT id, name
FROM items
ORDER BY embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;
```

**调整 probes**：
```sql
-- 调整查询时的探测数量（在查询前设置）
SET ivfflat.probes = 10;  -- 默认 1，增大可以提升准确度但降低速度

-- 执行查询
SELECT id, name
FROM items
ORDER BY embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;
```

---

### 步骤 4.4：索引性能对比

**目标**：对比有无索引的查询性能差异

**测试步骤**：
```sql
-- 1. 准备测试数据（插入 10,000+ 条记录）

-- 2. 测试无索引查询
DROP INDEX IF EXISTS items_embedding_idx;
EXPLAIN ANALYZE
SELECT id, name
FROM items
ORDER BY embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;

-- 3. 创建 HNSW 索引并测试
CREATE INDEX items_embedding_hnsw_idx ON items 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
EXPLAIN ANALYZE
SELECT id, name
FROM items
ORDER BY embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;

-- 4. 创建 IVFFlat 索引并测试（先删除 HNSW）
DROP INDEX items_embedding_hnsw_idx;
CREATE INDEX items_embedding_ivfflat_idx ON items 
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
EXPLAIN ANALYZE
SELECT id, name
FROM items
ORDER BY embedding <=> '[0.1,0.2,0.3,0.4,0.5]'::vector
LIMIT 10;
```

**记录结果**：
- 无索引查询时间
- HNSW 索引查询时间
- IVFFlat 索引查询时间
- 索引大小
- 索引构建时间

---

### 步骤 4.5：索引参数调优

**目标**：测试不同索引参数对性能的影响

**HNSW 参数调优**：
```sql
-- 测试不同的 m 值
CREATE INDEX idx1 ON items USING hnsw (embedding vector_cosine_ops) WITH (m = 8);
CREATE INDEX idx2 ON items USING hnsw (embedding vector_cosine_ops) WITH (m = 16);
CREATE INDEX idx3 ON items USING hnsw (embedding vector_cosine_ops) WITH (m = 32);
CREATE INDEX idx4 ON items USING hnsw (embedding vector_cosine_ops) WITH (m = 64);

-- 分别测试查询性能
SET hnsw.ef_search = 40;
EXPLAIN ANALYZE SELECT ...;
```

**IVFFlat 参数调优**：
```sql
-- 测试不同的 lists 值
CREATE INDEX idx1 ON items USING ivfflat (embedding vector_cosine_ops) WITH (lists = 50);
CREATE INDEX idx2 ON items USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
CREATE INDEX idx3 ON items USING ivfflat (embedding vector_cosine_ops) WITH (lists = 200);

-- 测试不同的 probes 值
SET ivfflat.probes = 1;
EXPLAIN ANALYZE SELECT ...;
SET ivfflat.probes = 10;
EXPLAIN ANALYZE SELECT ...;
SET ivfflat.probes = 100;
EXPLAIN ANALYZE SELECT ...;
```

**权衡**：
- **m / lists**：越大，索引越精确，但构建时间和存储空间越大
- **ef_construction / probes**：越大，查询越精确，但查询时间越长

---

## 🐍 第五阶段：Python 应用开发

### 步骤 5.1：环境准备

**目标**：安装 Python 依赖包

**依赖包**：
- `psycopg2-binary` 或 `psycopg2`：PostgreSQL 驱动
- `numpy`：向量计算
- `sentence-transformers`：文本向量化（可选）

**安装命令**：
```bash
pip install psycopg2-binary numpy
pip install sentence-transformers  # 用于文本向量化
```

**创建项目结构**：
```
04_python_integration/
├── requirements.txt
├── config.py          # 数据库配置
├── db_utils.py        # 数据库工具类
├── vector_ops.py      # 向量操作封装
├── examples/
│   ├── insert_example.py
│   ├── search_example.py
│   └── batch_example.py
└── README.md
```

---

### 步骤 5.2：数据库连接工具类

**目标**：封装数据库连接逻辑

**代码框架**：
```python
# db_utils.py
import psycopg2
from psycopg2.extras import RealDictCursor
from contextlib import contextmanager

class PgVectorClient:
    def __init__(self, host, port, database, user, password):
        self.conn_params = {
            'host': host,
            'port': port,
            'database': database,
            'user': user,
            'password': password
        }
    
    @contextmanager
    def get_connection(self):
        conn = psycopg2.connect(**self.conn_params)
        try:
            yield conn
        finally:
            conn.close()
    
    def execute_query(self, query, params=None):
        with self.get_connection() as conn:
            cur = conn.cursor(cursor_factory=RealDictCursor)
            cur.execute(query, params)
            return cur.fetchall()
    
    def execute_update(self, query, params=None):
        with self.get_connection() as conn:
            cur = conn.cursor()
            cur.execute(query, params)
            conn.commit()
            return cur.rowcount
```

---

### 步骤 5.3：向量插入函数封装

**目标**：封装向量插入操作

**代码框架**：
```python
# vector_ops.py
import numpy as np
from db_utils import PgVectorClient

class VectorOperations:
    def __init__(self, client: PgVectorClient):
        self.client = client
    
    def insert_vector(self, table_name, data_dict, embedding):
        """
        插入向量数据
        :param table_name: 表名
        :param data_dict: 其他字段的字典
        :param embedding: 向量（numpy array 或 list）
        """
        # 将向量转换为字符串格式
        if isinstance(embedding, np.ndarray):
            embedding = embedding.tolist()
        vector_str = '[' + ','.join(map(str, embedding)) + ']'
        
        # 构建插入 SQL
        fields = list(data_dict.keys()) + ['embedding']
        values = list(data_dict.values()) + [vector_str]
        placeholders = ', '.join(['%s'] * len(fields))
        fields_str = ', '.join(fields)
        
        query = f"INSERT INTO {table_name} ({fields_str}) VALUES ({placeholders})"
        # embedding 需要转换为 vector 类型
        query = query.replace("'%s'::vector", "%s::vector")
        
        self.client.execute_update(query, values)
    
    def batch_insert(self, table_name, data_list, embeddings):
        """
        批量插入向量数据
        """
        # 实现批量插入逻辑
        pass
```

---

### 步骤 5.4：相似度搜索函数封装

**目标**：封装相似度搜索操作

**代码框架**：
```python
# vector_ops.py
class VectorOperations:
    # ... 其他方法 ...
    
    def cosine_search(self, table_name, query_vector, limit=10, threshold=None):
        """
        余弦相似度搜索
        """
        if isinstance(query_vector, np.ndarray):
            query_vector = query_vector.tolist()
        vector_str = '[' + ','.join(map(str, query_vector)) + ']'
        
        query = f"""
            SELECT 
                id,
                1 - (embedding <=> %s::vector) AS similarity,
                *
            FROM {table_name}
        """
        
        if threshold:
            query += f" WHERE 1 - (embedding <=> %s::vector) > {threshold}"
        
        query += """
            ORDER BY embedding <=> %s::vector
            LIMIT %s
        """
        
        return self.client.execute_query(query, [vector_str, vector_str, vector_str, limit])
    
    def euclidean_search(self, table_name, query_vector, limit=10):
        """
        欧氏距离搜索
        """
        # 类似实现
        pass
```

---

### 步骤 5.5：使用 embedding 模型进行文本向量化

**目标**：集成文本向量化模型

**代码框架**：
```python
# embedding_utils.py
from sentence_transformers import SentenceTransformer
import numpy as np

class TextEmbedder:
    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
        self.model = SentenceTransformer(model_name)
    
    def encode(self, text):
        """
        将文本转换为向量
        """
        return self.model.encode(text)
    
    def encode_batch(self, texts):
        """
        批量向量化
        """
        return self.model.encode(texts)

# 使用示例
embedder = TextEmbedder()
text = "这是一个测试文本"
vector = embedder.encode(text)

# 插入到数据库
vector_ops = VectorOperations(client)
vector_ops.insert_vector('documents', {'content': text}, vector)
```

---

### 步骤 5.6：完整示例应用

**目标**：实现一个简单的文档检索系统

**功能**：
1. 文档上传和向量化
2. 向量存储
3. 问题检索相关文档

**代码框架**：
```python
# document_search.py
from vector_ops import VectorOperations
from embedding_utils import TextEmbedder
from db_utils import PgVectorClient

class DocumentSearch:
    def __init__(self, db_client, embedder):
        self.vector_ops = VectorOperations(db_client)
        self.embedder = embedder
    
    def add_document(self, content, metadata=None):
        """添加文档"""
        vector = self.embedder.encode(content)
        data = {'content': content}
        if metadata:
            data.update(metadata)
        self.vector_ops.insert_vector('documents', data, vector)
    
    def search(self, query, limit=5):
        """搜索相关文档"""
        query_vector = self.embedder.encode(query)
        results = self.vector_ops.cosine_search('documents', query_vector, limit)
        return results
```

---

## 🎓 第六阶段：综合项目实践

### 步骤 6.1：项目需求分析

**目标**：设计一个完整的文档问答系统（RAG 基础）

**功能需求**：
1. 文档上传（支持文本、PDF等）
2. 文档分块和向量化
3. 向量存储到 PgVector
4. 问题向量化后检索相关文档块
5. 返回 Top-K 相似结果
6. 提供 REST API 接口

---

### 步骤 6.2：数据库架构设计

**表结构**：
```sql
-- 文档表
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    title TEXT,
    content TEXT,
    file_path TEXT,
    file_type TEXT,
    chunk_index INTEGER,
    embedding vector(384),  -- 根据使用的模型调整
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 创建 HNSW 索引
CREATE INDEX ON documents 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

---

### 步骤 6.3：文档处理流程

**步骤**：
1. 文档上传 → 解析文本
2. 文本分块（chunking）
3. 每个块向量化
4. 批量插入到数据库

**代码框架**：
```python
# document_processor.py
from sentence_transformers import SentenceTransformer
import numpy as np

class DocumentProcessor:
    def __init__(self, chunk_size=500, chunk_overlap=50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.embedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    def chunk_text(self, text):
        """文本分块"""
        chunks = []
        start = 0
        while start < len(text):
            end = start + self.chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - self.chunk_overlap
        return chunks
    
    def process_document(self, text, title=None):
        """处理文档并返回向量化的块"""
        chunks = self.chunk_text(text)
        vectors = self.embedder.encode(chunks)
        return [
            {
                'chunk': chunk,
                'vector': vector,
                'index': i,
                'title': title
            }
            for i, (chunk, vector) in enumerate(zip(chunks, vectors))
        ]
```

---

### 步骤 6.4：API 接口实现

**使用 FastAPI**：
```python
# api.py
from fastapi import FastAPI, UploadFile, File
from document_search import DocumentSearch
from document_processor import DocumentProcessor

app = FastAPI()
doc_search = DocumentSearch(...)
processor = DocumentProcessor()

@app.post("/upload")
async def upload_document(file: UploadFile = File(...)):
    """上传文档"""
    content = await file.read()
    text = extract_text(content, file.filename)  # 根据文件类型解析
    
    chunks_data = processor.process_document(text, file.filename)
    for chunk_data in chunks_data:
        doc_search.add_document(chunk_data['chunk'], 
                              {'title': chunk_data['title'], 
                               'chunk_index': chunk_data['index']})
    return {"message": "Document uploaded successfully"}

@app.post("/search")
async def search_documents(query: str, limit: int = 5):
    """搜索文档"""
    results = doc_search.search(query, limit)
    return {"results": results}
```

---

### 步骤 6.5：性能优化

**优化点**：
1. **批量插入**：使用批量插入减少数据库交互
2. **连接池**：使用连接池管理数据库连接
3. **异步处理**：大文档处理使用异步任务
4. **缓存**：对频繁查询的结果进行缓存

---

### 步骤 6.6：测试和文档

**测试**：
- 单元测试：测试各个功能模块
- 集成测试：测试完整的文档检索流程
- 性能测试：测试查询速度和吞吐量

**文档**：
- API 文档（使用 FastAPI 自动生成）
- 使用说明
- 部署指南

---

## 📝 实践检查清单

### 第一阶段
- [ ] Docker 环境检查完成
- [ ] 数据库连接成功
- [ ] 理解向量数据库基础概念

### 第二阶段
- [ ] pgvector 扩展安装成功
- [ ] 创建测试数据库
- [ ] 创建包含向量列的表
- [ ] 插入测试数据

### 第三阶段
- [ ] 实现向量插入操作
- [ ] 实现余弦相似度搜索
- [ ] 实现欧氏距离搜索
- [ ] 实现带阈值的搜索
- [ ] 实现混合搜索

### 第四阶段
- [ ] 创建 HNSW 索引
- [ ] 创建 IVFFlat 索引
- [ ] 对比索引性能
- [ ] 参数调优测试

### 第五阶段
- [ ] Python 环境搭建
- [ ] 数据库连接工具类完成
- [ ] 向量操作封装完成
- [ ] 集成 embedding 模型
- [ ] 实现简单应用

### 第六阶段
- [ ] 项目需求分析完成
- [ ] 数据库架构设计完成
- [ ] 文档处理流程实现
- [ ] API 接口实现
- [ ] 性能优化完成
- [ ] 测试和文档完成

---

**祝你学习顺利！遇到问题随时查阅文档和社区资源。** 🚀

