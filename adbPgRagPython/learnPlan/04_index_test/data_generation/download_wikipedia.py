"""
Wikipedia æ•°æ®ä¸‹è½½å·¥å…·
æ”¯æŒä» Wikipedia ä¸‹è½½æ–‡æœ¬æ•°æ®å¹¶å¤„ç†ï¼Œç”¨äºç”Ÿæˆæ–‡æœ¬å‘é‡
"""

import sys
import argparse
import json
import re
import time
from pathlib import Path
from typing import List, Iterator, Optional
import urllib.request
import urllib.parse
import urllib.error
from tqdm import tqdm


class WikipediaDownloader:
    """
    Wikipedia æ•°æ®ä¸‹è½½å·¥å…·ç±»
    æ”¯æŒé€šè¿‡ API ä¸‹è½½ Wikipedia æ–‡æœ¬æ•°æ®
    """
    
    # Wikipedia API ç«¯ç‚¹
    API_BASE_URL = "https://{lang}.wikipedia.org/w/api.php"
    
    def __init__(self, language: str = "zh", timeout: int = 30, retry_times: int = 3):
        """
        åˆå§‹åŒ– Wikipedia ä¸‹è½½å™¨
        
        Args:
            language: Wikipedia è¯­è¨€ä»£ç ï¼ˆé»˜è®¤ï¼šzh ä¸­æ–‡ï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ï¼š30ï¼‰
            retry_times: å¤±è´¥é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ï¼š3ï¼‰
        """
        self.language = language
        self.api_url = self.API_BASE_URL.format(lang=language)
        self.timeout = timeout
        self.retry_times = retry_times
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼ˆé¿å…æŸäº›ç½‘ç»œç¯å¢ƒçš„é—®é¢˜ï¼‰
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def _make_request(self, url: str) -> Optional[dict]:
        """
        å‘èµ· HTTP è¯·æ±‚ï¼ˆå¸¦è¶…æ—¶å’Œé‡è¯•ï¼‰
        
        Args:
            url: è¯·æ±‚ URL
        
        Returns:
            Optional[dict]: è§£æåçš„ JSON æ•°æ®ï¼Œå¤±è´¥è¿”å› None
        """
        request = urllib.request.Request(url, headers=self.headers)
        
        for attempt in range(self.retry_times):
            try:
                with urllib.request.urlopen(request, timeout=self.timeout) as response:
                    data = json.loads(response.read().decode('utf-8'))
                    return data
            except urllib.error.URLError as e:
                if attempt < self.retry_times - 1:
                    wait_time = (attempt + 1) * 2  # é€’å¢ç­‰å¾…æ—¶é—´
                    print(f"âš ï¸  è¯·æ±‚å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯•... ({str(e)})")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ˆå·²é‡è¯•{self.retry_times}æ¬¡ï¼‰: {str(e)}")
                    return None
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")
                return None
        
        return None
    
    def search_pages(self, search_term: str, limit: int = 10) -> List[dict]:
        """
        æœç´¢ Wikipedia é¡µé¢
        
        Args:
            search_term: æœç´¢å…³é”®è¯
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
        
        Returns:
            List[dict]: é¡µé¢ä¿¡æ¯åˆ—è¡¨
        """
        params = {
            'action': 'query',
            'list': 'search',
            'srsearch': search_term,
            'srlimit': limit,
            'format': 'json'
        }
        
        url = f"{self.api_url}?{urllib.parse.urlencode(params)}"
        data = self._make_request(url)
        
        if not data:
            return []
        
        pages = []
        if 'query' in data and 'search' in data['query']:
            for item in data['query']['search']:
                pages.append({
                    'title': item['title'],
                    'snippet': item['snippet']
                })
        
        return pages
    
    def get_random_pages(self, count: int = 10) -> List[dict]:
        """
        è·å–éšæœº Wikipedia é¡µé¢
        
        Args:
            count: è·å–é¡µé¢æ•°é‡
        
        Returns:
            List[dict]: é¡µé¢æ ‡é¢˜åˆ—è¡¨
        """
        params = {
            'action': 'query',
            'list': 'random',
            'rnnamespace': 0,  # åªè·å–ä¸»å‘½åç©ºé—´çš„é¡µé¢
            'rnlimit': min(count, 500),  # API é™åˆ¶æœ€å¤š 500
            'format': 'json'
        }
        
        url = f"{self.api_url}?{urllib.parse.urlencode(params)}"
        data = self._make_request(url)
        
        if not data:
            return []
        
        pages = []
        if 'query' in data and 'random' in data['query']:
            for item in data['query']['random']:
                pages.append({
                    'title': item['title'],
                    'id': item['id']
                })
        
        return pages
    
    def get_page_content(self, page_title: str) -> Optional[str]:
        """
        è·å–é¡µé¢å†…å®¹
        
        Args:
            page_title: é¡µé¢æ ‡é¢˜
        
        Returns:
            Optional[str]: é¡µé¢æ–‡æœ¬å†…å®¹ï¼Œå¤±è´¥è¿”å› None
        """
        params = {
            'action': 'query',
            'prop': 'extracts',
            'exintro': False,  # è·å–å®Œæ•´å†…å®¹ï¼Œä¸åªæ˜¯ä»‹ç»
            'explaintext': True,  # çº¯æ–‡æœ¬æ ¼å¼
            'titles': page_title,
            'format': 'json'
        }
        
        url = f"{self.api_url}?{urllib.parse.urlencode(params)}"
        data = self._make_request(url)
        
        if not data:
            return None
        
        if 'query' in data and 'pages' in data['query']:
            pages = data['query']['pages']
            for page_id, page_data in pages.items():
                if 'extract' in page_data and page_data['extract']:
                    return page_data['extract']
        
        return None
    
    def download_pages(self, page_titles: List[str], output_file: str, 
                       chunk_size: int = 512, chunk_overlap: int = 0,
                       verbose: bool = True) -> int:
        """
        ä¸‹è½½å¤šä¸ªé¡µé¢çš„å†…å®¹å¹¶ä¿å­˜
        
        Args:
            page_titles: é¡µé¢æ ‡é¢˜åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°ï¼ˆ0 è¡¨ç¤ºä¸åˆ†å—ï¼‰
            chunk_overlap: åˆ†å—é‡å å¤§å°
            verbose: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        
        Returns:
            int: æˆåŠŸä¸‹è½½çš„é¡µé¢æ•°é‡
        """
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        texts = []
        success_count = 0
        
        if verbose:
            pbar = tqdm(total=len(page_titles), desc="ä¸‹è½½é¡µé¢")
        
        for title in page_titles:
            content = self.get_page_content(title)
            if content:
                if chunk_size > 0:
                    # åˆ†å—å¤„ç†
                    chunks = self._chunk_text(content, chunk_size, chunk_overlap)
                    texts.extend(chunks)
                else:
                    # ç›´æ¥ä½¿ç”¨å®Œæ•´å†…å®¹
                    texts.append(content)
                success_count += 1
            
            if verbose:
                pbar.update(1)
        
        if verbose:
            pbar.close()
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            for text in texts:
                # æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤å¤šä½™çš„ç©ºç™½ï¼‰
                cleaned_text = re.sub(r'\s+', ' ', text.strip())
                if cleaned_text:
                    f.write(cleaned_text + '\n')
        
        print(f"âœ… æˆåŠŸä¸‹è½½ {success_count}/{len(page_titles)} ä¸ªé¡µé¢")
        print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")
        print(f"   æ–‡æœ¬å—æ•°é‡: {len(texts)}")
        
        return success_count
    
    def download_random(self, count: int, output_file: str,
                       chunk_size: int = 512, chunk_overlap: int = 0,
                       batch_size: int = 50) -> int:
        """
        ä¸‹è½½éšæœºé¡µé¢
        
        Args:
            count: ä¸‹è½½é¡µé¢æ•°é‡
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°ï¼ˆ0 è¡¨ç¤ºä¸åˆ†å—ï¼‰
            chunk_overlap: åˆ†å—é‡å å¤§å°
            batch_size: æ‰¹é‡å¤„ç†å¤§å°ï¼ˆæ¯æ¬¡ API è¯·æ±‚è·å–çš„é¡µé¢æ•°ï¼‰
        
        Returns:
            int: æˆåŠŸä¸‹è½½çš„é¡µé¢æ•°é‡
        """
        all_titles = []
        
        print(f"æ­£åœ¨è·å– {count} ä¸ªéšæœºé¡µé¢æ ‡é¢˜...")
        total_batches = (count + batch_size - 1) // batch_size
        
        with tqdm(total=count, desc="è·å–é¡µé¢æ ‡é¢˜") as pbar:
            for i in range(0, count, batch_size):
                batch_count = min(batch_size, count - i)
                print(f"\nğŸ“¥ æ‰¹é‡è·å– {batch_count} ä¸ªé¡µé¢æ ‡é¢˜ ({i//batch_size + 1}/{total_batches})...")
                pages = self.get_random_pages(batch_count)
                
                if pages:
                    all_titles.extend([p['title'] for p in pages])
                    pbar.update(len(pages))
                else:
                    print(f"âš ï¸  æ‰¹é‡ {i//batch_size + 1} è·å–å¤±è´¥ï¼Œè·³è¿‡")
                    pbar.update(batch_count)  # ä»ç„¶æ›´æ–°è¿›åº¦ï¼Œé¿å…å¡ä½
        
        if not all_titles:
            print("âŒ æœªèƒ½è·å–åˆ°ä»»ä½•é¡µé¢æ ‡é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            return 0
        
        print(f"\nâœ… è·å–äº† {len(all_titles)} ä¸ªé¡µé¢æ ‡é¢˜")
        
        # ä¸‹è½½å†…å®¹
        return self.download_pages(all_titles, output_file, chunk_size, chunk_overlap)
    
    def list_category_members(self, category: str, limit: int = 500, 
                             subcategories_only: bool = False) -> dict:
        """
        åˆ—å‡ºåˆ†ç±»ä¸‹çš„æˆå‘˜ï¼ˆåŒ…æ‹¬å­åˆ†ç±»å’Œé¡µé¢ï¼‰
        
        Args:
            category: åˆ†ç±»åç§°ï¼ˆå¦‚ "Category:åŒ»ç–—"ï¼‰
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            subcategories_only: æ˜¯å¦åªè¿”å›å­åˆ†ç±»
        
        Returns:
            dict: åŒ…å« 'subcategories' å’Œ 'pages' çš„å­—å…¸
        """
        params = {
            'action': 'query',
            'list': 'categorymembers',
            'cmtitle': category,
            'cmlimit': limit,
            'format': 'json'
        }
        
        url = f"{self.api_url}?{urllib.parse.urlencode(params)}"
        data = self._make_request(url)
        
        if not data:
            return {'subcategories': [], 'pages': []}
        
        subcategories = []
        pages = []
        
        if 'query' in data and 'categorymembers' in data['query']:
            for item in data['query']['categorymembers']:
                if item['title'].startswith('Category:'):
                    subcategories.append(item['title'])
                else:
                    pages.append(item['title'])
        
        if subcategories_only:
            return {'subcategories': subcategories, 'pages': []}
        
        return {'subcategories': subcategories, 'pages': pages}
    
    def search_categories(self, keyword: str, limit: int = 50) -> List[str]:
        """
        æœç´¢åŒ…å«å…³é”®è¯çš„åˆ†ç±»
        
        Args:
            keyword: æœç´¢å…³é”®è¯ï¼ˆå¦‚ "åŒ»ç–—"ã€"åŒ»å­¦"ï¼‰
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
        
        Returns:
            List[str]: åŒ¹é…çš„åˆ†ç±»åç§°åˆ—è¡¨
        """
        categories = []
        
        # æ–¹æ³•1ï¼šå°è¯•åœ¨Categoryå‘½åç©ºé—´æœç´¢
        params1 = {
            'action': 'query',
            'list': 'search',
            'srsearch': keyword,
            'srnamespace': 14,  # å‘½åç©ºé—´14æ˜¯Categoryå‘½åç©ºé—´
            'srlimit': limit,
            'format': 'json'
        }
        
        url1 = f"{self.api_url}?{urllib.parse.urlencode(params1)}"
        data1 = self._make_request(url1)
        
        if data1 and 'query' in data1 and 'search' in data1['query']:
            for item in data1['query']['search']:
                if item['title'].startswith('Category:'):
                    categories.append(item['title'])
        
        # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1æ²¡ç»“æœï¼Œå°è¯•æœç´¢"Category:å…³é”®è¯"
        if not categories:
            params2 = {
                'action': 'query',
                'list': 'search',
                'srsearch': f"Category:{keyword}",
                'srlimit': limit,
                'format': 'json'
            }
            
            url2 = f"{self.api_url}?{urllib.parse.urlencode(params2)}"
            data2 = self._make_request(url2)
            
            if data2 and 'query' in data2 and 'search' in data2['query']:
                for item in data2['query']['search']:
                    if item['title'].startswith('Category:'):
                        if item['title'] not in categories:
                            categories.append(item['title'])
        
        return categories
    
    def get_subcategories(self, category: str, limit: int = 500) -> List[str]:
        """
        è·å–æŸä¸ªåˆ†ç±»çš„å­åˆ†ç±»
        
        Args:
            category: åˆ†ç±»åç§°ï¼ˆå¦‚ "Category:åŒ»ç–—"ï¼‰
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
        
        Returns:
            List[str]: å­åˆ†ç±»åç§°åˆ—è¡¨
        """
        result = self.list_category_members(category, limit, subcategories_only=True)
        return result['subcategories']
    
    def download_by_category(self, category: str, output_file: str,
                            limit: int = 100, chunk_size: int = 512,
                            chunk_overlap: int = 0) -> int:
        """
        æŒ‰åˆ†ç±»ä¸‹è½½é¡µé¢
        
        Args:
            category: åˆ†ç±»åç§°ï¼ˆå¦‚ "Category:è®¡ç®—æœºç§‘å­¦"ï¼‰
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            limit: é™åˆ¶ä¸‹è½½æ•°é‡
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
        
        Returns:
            int: æˆåŠŸä¸‹è½½çš„é¡µé¢æ•°é‡
        """
        params = {
            'action': 'query',
            'list': 'categorymembers',
            'cmtitle': category,
            'cmnamespace': 0,  # åªè·å–ä¸»å‘½åç©ºé—´çš„é¡µé¢ï¼Œä¸åŒ…æ‹¬å­åˆ†ç±»
            'cmlimit': limit,
            'format': 'json'
        }
        
        url = f"{self.api_url}?{urllib.parse.urlencode(params)}"
        data = self._make_request(url)
        
        if not data:
            return 0
        
        titles = []
        if 'query' in data and 'categorymembers' in data['query']:
            for item in data['query']['categorymembers']:
                # åªä¸‹è½½é¡µé¢ï¼Œè·³è¿‡å­åˆ†ç±»
                if not item['title'].startswith('Category:'):
                    titles.append(item['title'])
        
        print(f"âœ… æ‰¾åˆ° {len(titles)} ä¸ªé¡µé¢")
        
        return self.download_pages(titles, output_file, chunk_size, chunk_overlap)
    
    def download_by_search(self, search_term: str, output_file: str,
                          limit: int = 50, chunk_size: int = 512,
                          chunk_overlap: int = 0) -> int:
        """
        æŒ‰æœç´¢å…³é”®è¯ä¸‹è½½é¡µé¢
        
        Args:
            search_term: æœç´¢å…³é”®è¯
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            limit: é™åˆ¶ä¸‹è½½æ•°é‡
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å å¤§å°
        
        Returns:
            int: æˆåŠŸä¸‹è½½çš„é¡µé¢æ•°é‡
        """
        print(f"æœç´¢å…³é”®è¯: {search_term}")
        pages = self.search_pages(search_term, limit)
        
        if not pages:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³é¡µé¢")
            return 0
        
        titles = [p['title'] for p in pages]
        print(f"âœ… æ‰¾åˆ° {len(titles)} ä¸ªé¡µé¢")
        
        return self.download_pages(titles, output_file, chunk_size, chunk_overlap)
    
    @staticmethod
    def _chunk_text(text: str, chunk_size: int, overlap: int = 0) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å—
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            chunk_size: æ¯å—å­—ç¬¦æ•°
            overlap: é‡å å­—ç¬¦æ•°
        
        Returns:
            List[str]: æ–‡æœ¬å—åˆ—è¡¨
        """
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            if chunk.strip():
                chunks.append(chunk.strip())
            start = end - overlap
            if start >= len(text):
                break
        
        return chunks


def main():
    # é»˜è®¤è¾“å‡ºç›®å½•
    default_output_dir = Path(__file__).parent / 'download_wikipedia'
    default_output_dir.mkdir(parents=True, exist_ok=True)
    
    parser = argparse.ArgumentParser(description='ä» Wikipedia ä¸‹è½½æ–‡æœ¬æ•°æ®æˆ–æŸ¥æ‰¾åˆ†ç±»')
    parser.add_argument('--language', type=str, default='zh',
                       help='Wikipedia è¯­è¨€ä»£ç ï¼ˆé»˜è®¤ï¼šzh ä¸­æ–‡ï¼‰')
    parser.add_argument('--output', type=str, default=None,
                       help=f'è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆ.txt æ ¼å¼ï¼Œé»˜è®¤ï¼šä¿å­˜åœ¨ {default_output_dir} ç›®å½•ï¼‰')
    parser.add_argument('--timeout', type=int, default=30,
                       help='è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ï¼š30ï¼‰')
    parser.add_argument('--retry', type=int, default=3,
                       help='å¤±è´¥é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ï¼š3ï¼‰')
    
    # æŸ¥æ‰¾åˆ†ç±»é€‰é¡¹ï¼ˆä¸ä¸‹è½½é€‰é¡¹äº’æ–¥ï¼‰
    find_group = parser.add_argument_group('æŸ¥æ‰¾åˆ†ç±»é€‰é¡¹')
    find_group.add_argument('--find-categories', type=str, metavar='KEYWORD',
                           help='æœç´¢åŒ…å«å…³é”®è¯çš„åˆ†ç±»ï¼ˆå¦‚ï¼š"åŒ»ç–—"ã€"åŒ»å­¦"ï¼‰')
    find_group.add_argument('--list-category', type=str, metavar='CATEGORY',
                           help='åˆ—å‡ºåˆ†ç±»ä¸‹çš„æˆå‘˜å’Œå­åˆ†ç±»ï¼ˆå¦‚ï¼š"Category:åŒ»ç–—"ï¼‰')
    find_group.add_argument('--list-subcategories', type=str, metavar='CATEGORY',
                           help='åˆ—å‡ºåˆ†ç±»çš„å­åˆ†ç±»ï¼ˆå¦‚ï¼š"Category:åŒ»ç–—"ï¼‰')
    find_group.add_argument('--find-limit', type=int, default=50,
                           help='æŸ¥æ‰¾ç»“æœæ˜¾ç¤ºæ•°é‡é™åˆ¶ï¼ˆé»˜è®¤ï¼š50ï¼‰')
    
    # æ•°æ®æºé€‰é¡¹ï¼ˆäº’æ–¥ï¼‰
    data_source_group = parser.add_mutually_exclusive_group(required=False)
    data_source_group.add_argument('--random', type=int,
                                   help='ä¸‹è½½éšæœºé¡µé¢ï¼ˆæŒ‡å®šæ•°é‡ï¼‰')
    data_source_group.add_argument('--search', type=str,
                                   help='æŒ‰å…³é”®è¯æœç´¢å¹¶ä¸‹è½½')
    data_source_group.add_argument('--category', type=str,
                                   help='æŒ‰åˆ†ç±»ä¸‹è½½ï¼ˆå¦‚ "Category:è®¡ç®—æœºç§‘å­¦"ï¼‰')
    data_source_group.add_argument('--titles', type=str,
                                   help='æŒ‡å®šé¡µé¢æ ‡é¢˜åˆ—è¡¨æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªæ ‡é¢˜ï¼‰')
    
    parser.add_argument('--limit', type=int, default=100,
                       help='é™åˆ¶ä¸‹è½½æ•°é‡ï¼ˆé»˜è®¤ï¼š100ï¼Œä»…ç”¨äº --search å’Œ --categoryï¼‰')
    parser.add_argument('--chunk-size', type=int, default=512,
                       help='æ–‡æœ¬åˆ†å—å¤§å°ï¼ˆ0 è¡¨ç¤ºä¸åˆ†å—ï¼Œé»˜è®¤ï¼š512ï¼‰')
    parser.add_argument('--chunk-overlap', type=int, default=0,
                       help='æ–‡æœ¬åˆ†å—é‡å å¤§å°ï¼ˆé»˜è®¤ï¼š0ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦åªæ˜¯æŸ¥æ‰¾æ“ä½œ
    is_find_mode = args.find_categories or args.list_category or args.list_subcategories
    
    # å¦‚æœæ˜¯æŸ¥æ‰¾æ¨¡å¼ï¼Œä¸éœ€è¦ä¸‹è½½å‚æ•°
    if is_find_mode:
        # åˆ›å»ºä¸‹è½½å™¨
        downloader = WikipediaDownloader(
            language=args.language,
            timeout=args.timeout,
            retry_times=args.retry
        )
        
        print("="*60)
        print("Wikipedia åˆ†ç±»æŸ¥æ‰¾å·¥å…·")
        print("="*60)
        print(f"è¯­è¨€: {args.language}\n")
        
        try:
            # æœç´¢åˆ†ç±»
            if args.find_categories:
                print(f"ğŸ” æœç´¢åŒ…å«å…³é”®è¯ '{args.find_categories}' çš„åˆ†ç±»...\n")
                categories = downloader.search_categories(args.find_categories, args.find_limit)
                
                if categories:
                    print(f"âœ… æ‰¾åˆ° {len(categories)} ä¸ªç›¸å…³åˆ†ç±»ï¼š\n")
                    for i, cat in enumerate(categories, 1):
                        cat_name = cat.replace('Category:', '')
                        print(f"  {i}. {cat}")
                        print(f"     ä½¿ç”¨å‘½ä»¤ä¸‹è½½: --category \"{cat}\" --limit 100\n")
                else:
                    print("âŒ æœªæ‰¾åˆ°ç›¸å…³åˆ†ç±»")
                    print("\nğŸ’¡ æç¤ºï¼š")
                    print("  - å°è¯•å…¶ä»–å…³é”®è¯ï¼ˆå¦‚ï¼š'åŒ»å­¦'ã€'å¥åº·'ã€'ç–¾ç—…'ï¼‰")
                    print("  - æˆ–ç›´æ¥åœ¨Wikipediaç½‘ç«™æµè§ˆåˆ†ç±»é¡µé¢")
                    print("  - Wikipediaåˆ†ç±»é¡µé¢: https://zh.wikipedia.org/wiki/Category:é¦–é¡µ")
            
            # åˆ—å‡ºåˆ†ç±»æˆå‘˜
            elif args.list_category:
                print(f"ğŸ“‹ åˆ—å‡ºåˆ†ç±» '{args.list_category}' çš„æˆå‘˜...\n")
                result = downloader.list_category_members(args.list_category, args.find_limit)
                
                subcategories = result['subcategories']
                pages = result['pages']
                
                print(f"ğŸ“‚ å­åˆ†ç±»ï¼ˆ{len(subcategories)} ä¸ªï¼‰ï¼š")
                if subcategories:
                    for i, subcat in enumerate(subcategories[:20], 1):  # æœ€å¤šæ˜¾ç¤º20ä¸ª
                        subcat_name = subcat.replace('Category:', '')
                        print(f"  {i}. {subcat}")
                        print(f"     ä½¿ç”¨å‘½ä»¤ä¸‹è½½: --category \"{subcat}\" --limit 100")
                    if len(subcategories) > 20:
                        print(f"  ... è¿˜æœ‰ {len(subcategories) - 20} ä¸ªå­åˆ†ç±»")
                else:
                    print("  ï¼ˆæ— å­åˆ†ç±»ï¼‰")
                
                print(f"\nğŸ“„ é¡µé¢ï¼ˆ{len(pages)} ä¸ªï¼Œæ˜¾ç¤ºå‰20ä¸ªï¼‰ï¼š")
                if pages:
                    for i, page in enumerate(pages[:20], 1):
                        print(f"  {i}. {page}")
                    if len(pages) > 20:
                        print(f"  ... è¿˜æœ‰ {len(pages) - 20} ä¸ªé¡µé¢")
                    print(f"\nğŸ’¡ ä¸‹è½½è¯¥åˆ†ç±»çš„æ‰€æœ‰é¡µé¢ï¼š")
                    print(f"   python data_generation/download_wikipedia.py --category \"{args.list_category}\" --limit {min(len(pages), 500)}")
                else:
                    print("  ï¼ˆæ— é¡µé¢ï¼‰")
            
            # åˆ—å‡ºå­åˆ†ç±»
            elif args.list_subcategories:
                print(f"ğŸ“‚ åˆ—å‡ºåˆ†ç±» '{args.list_subcategories}' çš„å­åˆ†ç±»...\n")
                subcategories = downloader.get_subcategories(args.list_subcategories, args.find_limit)
                
                if subcategories:
                    print(f"âœ… æ‰¾åˆ° {len(subcategories)} ä¸ªå­åˆ†ç±»ï¼š\n")
                    for i, subcat in enumerate(subcategories, 1):
                        subcat_name = subcat.replace('Category:', '')
                        print(f"  {i}. {subcat}")
                        print(f"     ä½¿ç”¨å‘½ä»¤ä¸‹è½½: --category \"{subcat}\" --limit 100\n")
                else:
                    print("âŒ è¯¥åˆ†ç±»æ²¡æœ‰å­åˆ†ç±»")
                    print(f"\nğŸ’¡ æŸ¥çœ‹è¯¥åˆ†ç±»ä¸‹çš„é¡µé¢ï¼š")
                    print(f"   python data_generation/download_wikipedia.py --list-category \"{args.list_subcategories}\"")
            
            print("\n" + "="*60)
            
        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        except Exception as e:
            print(f"\nâŒ æ“ä½œå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
        
        return
    
    # å¦‚æœä¸æ˜¯æŸ¥æ‰¾æ¨¡å¼ï¼Œéœ€è¦ä¸‹è½½å‚æ•°
    if not (args.random or args.search or args.category or args.titles):
        parser.error("å¿…é¡»æŒ‡å®šä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€ï¼š--random, --search, --category, --titles, "
                    "æˆ–æŸ¥æ‰¾é€‰é¡¹ï¼š--find-categories, --list-category, --list-subcategories")
    
    # å¦‚æœæœªæŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
    if args.output is None:
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if args.random:
            filename = f"wikipedia_random_{args.random}_{timestamp}.txt"
        elif args.search:
            # æ¸…ç†æœç´¢å…³é”®è¯ï¼Œç”¨äºæ–‡ä»¶å
            safe_search = re.sub(r'[^\w\s-]', '', args.search).strip()[:20]
            filename = f"wikipedia_search_{safe_search}_{timestamp}.txt"
        elif args.category:
            # æ¸…ç†åˆ†ç±»åï¼Œç”¨äºæ–‡ä»¶å
            safe_category = re.sub(r'[^\w\s-]', '', args.category.replace('Category:', '')).strip()[:20]
            filename = f"wikipedia_category_{safe_category}_{timestamp}.txt"
        elif args.titles:
            filename = f"wikipedia_titles_{timestamp}.txt"
        else:
            filename = f"wikipedia_{timestamp}.txt"
        
        args.output = str(default_output_dir / filename)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    print("="*60)
    print("Wikipedia æ•°æ®ä¸‹è½½å·¥å…·")
    print("="*60)
    print(f"è¯­è¨€: {args.language}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
    
    if args.random:
        print(f"æ¨¡å¼: éšæœºä¸‹è½½")
        print(f"æ•°é‡: {args.random}")
    elif args.search:
        print(f"æ¨¡å¼: å…³é”®è¯æœç´¢")
        print(f"å…³é”®è¯: {args.search}")
        print(f"é™åˆ¶: {args.limit}")
    elif args.category:
        print(f"æ¨¡å¼: åˆ†ç±»ä¸‹è½½")
        print(f"åˆ†ç±»: {args.category}")
        print(f"é™åˆ¶: {args.limit}")
    elif args.titles:
        print(f"æ¨¡å¼: æŒ‡å®šæ ‡é¢˜åˆ—è¡¨")
        print(f"æ–‡ä»¶: {args.titles}")
    
    print(f"æ–‡æœ¬åˆ†å—: {'æ˜¯' if args.chunk_size > 0 else 'å¦'}")
    if args.chunk_size > 0:
        print(f"  åˆ†å—å¤§å°: {args.chunk_size}")
        print(f"  é‡å å¤§å°: {args.chunk_overlap}")
    print("="*60)
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = WikipediaDownloader(
        language=args.language,
        timeout=args.timeout,
        retry_times=args.retry
    )
    
    # æ‰§è¡Œä¸‹è½½
    success_count = 0
    
    try:
        if args.random:
            success_count = downloader.download_random(
                args.random, args.output,
                args.chunk_size, args.chunk_overlap
            )
        elif args.search:
            success_count = downloader.download_by_search(
                args.search, args.output, args.limit,
                args.chunk_size, args.chunk_overlap
            )
        elif args.category:
            success_count = downloader.download_by_category(
                args.category, args.output, args.limit,
                args.chunk_size, args.chunk_overlap
            )
        elif args.titles:
            # ä»æ–‡ä»¶è¯»å–æ ‡é¢˜åˆ—è¡¨
            with open(args.titles, 'r', encoding='utf-8') as f:
                titles = [line.strip() for line in f if line.strip()]
            
            print(f"âœ… è¯»å–äº† {len(titles)} ä¸ªé¡µé¢æ ‡é¢˜")
            success_count = downloader.download_pages(
                titles, args.output,
                args.chunk_size, args.chunk_overlap
            )
        
        print("\n" + "="*60)
        print("âœ… ä¸‹è½½å®Œæˆï¼")
        print("="*60)
        print(f"æˆåŠŸä¸‹è½½: {success_count} ä¸ªé¡µé¢")
        print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼šä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç”Ÿæˆå‘é‡")
        print(f"   python data_generation/generate_text_vectors.py --file {args.output}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

