"""
ç”ŸæˆçœŸå®æ–‡æœ¬å‘é‡æ•°æ®
ä½¿ç”¨ embedding æ¨¡å‹å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
ç”¨äºç´¢å¼•æ€§èƒ½æµ‹è¯•å’ŒæŸ¥è¯¢ç²¾åº¦è¯„ä¼°
"""

import sys
import argparse
import json
import importlib.util
import os
import urllib.request
import gzip
from pathlib import Path
from typing import List, Tuple, Iterator, Optional
import numpy as np
from tqdm import tqdm

# æ˜ç¡®å¯¼å…¥ 04_index_test çš„ configï¼ˆé¿å…ä¸ 05_simple_test çš„ config å†²çªï¼‰
_04_index_test_path = Path(__file__).parent.parent
_config_path = _04_index_test_path / 'config.py'
spec = importlib.util.spec_from_file_location("_config_04", _config_path)
_config_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(_config_module)
DB_CONFIG = _config_module.DB_CONFIG
MODEL_DIMENSION = _config_module.MODEL_DIMENSION
TEST_TABLE_NAME = _config_module.TEST_TABLE_NAME

# æ·»åŠ  05_simple_test è·¯å¾„ï¼ˆç”¨äºå¯¼å…¥ embedding_utils å’Œ db_utilsï¼‰
_05_simple_test_path = Path(__file__).parent.parent.parent / '05_simple_test'
sys.path.insert(0, str(_05_simple_test_path))

from db_utils import PgVectorClient
from embedding_utils import TextEmbedder


def read_text_file(file_path: str, encoding: str = 'utf-8') -> List[str]:
    """
    ä»æ–‡æœ¬æ–‡ä»¶è¯»å–å†…å®¹
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        encoding: æ–‡ä»¶ç¼–ç 
    
    Returns:
        List[str]: æ–‡æœ¬åˆ—è¡¨ï¼ˆæŒ‰è¡Œåˆ†å‰²ï¼Œè¿‡æ»¤ç©ºè¡Œï¼‰
    """
    texts = []
    try:
        with open(file_path, 'r', encoding=encoding) as f:
            for line in f:
                line = line.strip()
                if line:  # è¿‡æ»¤ç©ºè¡Œ
                    texts.append(line)
    except UnicodeDecodeError:
        # å¦‚æœ UTF-8 å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
        with open(file_path, 'r', encoding='gbk') as f:
            for line in f:
                line = line.strip()
                if line:
                    texts.append(line)
    return texts


def read_text_directory(dir_path: str, extensions: List[str] = None) -> Iterator[Tuple[str, str]]:
    """
    ä»ç›®å½•è¯»å–æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
    
    Args:
        dir_path: ç›®å½•è·¯å¾„
        extensions: æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼ˆå¦‚ ['.txt', '.md']ï¼‰ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰æ–‡ä»¶
    
    Yields:
        Tuple[str, str]: (æ–‡ä»¶è·¯å¾„, æ–‡ä»¶å†…å®¹)
    """
    if extensions is None:
        extensions = ['.txt', '.md', '.py', '.java', '.json', '.csv']
    
    dir_path_obj = Path(dir_path)
    if not dir_path_obj.exists():
        raise ValueError(f"ç›®å½•ä¸å­˜åœ¨: {dir_path}")
    
    for file_path in dir_path_obj.rglob('*'):
        if file_path.is_file() and file_path.suffix in extensions:
            try:
                content = file_path.read_text(encoding='utf-8')
                if content.strip():
                    yield (str(file_path), content)
            except Exception as e:
                print(f"âš ï¸  è·³è¿‡æ–‡ä»¶ {file_path}: {str(e)}")
                continue


def download_wikipedia_sample(output_file: str, limit: int = 10000):
    """
    ä¸‹è½½ Wikipedia ç¤ºä¾‹æ–‡æœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰
    æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹å®ç°ï¼Œå®é™…ä½¿ç”¨æ—¶å»ºè®®ï¼š
    1. ä½¿ç”¨ wikimedia API ä¸‹è½½
    2. æˆ–ä½¿ç”¨å·²å¤„ç†å¥½çš„ Wikipedia æ–‡æœ¬æ•°æ®é›†
    
    Args:
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        limit: é™åˆ¶æ–‡æœ¬æ•°é‡
    """
    print("âš ï¸  è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹å®ç°ï¼Œå®é™…ä½¿ç”¨æ—¶å»ºè®®ä½¿ç”¨å·²å¤„ç†å¥½çš„ Wikipedia æ•°æ®é›†")
    print("ğŸ“ å»ºè®®æ–¹æ³•ï¼š")
    print("   1. ä» https://dumps.wikimedia.org/ ä¸‹è½½ Wikipedia è½¬å‚¨")
    print("   2. ä½¿ç”¨å·¥å…·ï¼ˆå¦‚ WikiExtractorï¼‰æå–çº¯æ–‡æœ¬")
    print("   3. å°†æ–‡æœ¬ä¿å­˜ä¸ºæ–‡ä»¶ï¼Œä½¿ç”¨ --file æˆ– --dir å‚æ•°")
    print(f"\nğŸ’¡ å½“å‰å°†åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ–‡ä»¶ï¼š{output_file}")
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æœ¬æ–‡ä»¶
    sample_texts = generate_sample_texts(min(limit, 100))
    with open(output_file, 'w', encoding='utf-8') as f:
        for text in sample_texts:
            f.write(text + '\n')
    
    print(f"âœ… å·²åˆ›å»ºç¤ºä¾‹æ–‡ä»¶ï¼š{output_file}")
    return output_file


def load_precomputed_vectors(vector_file: str, limit: Optional[int] = None) -> Tuple[np.ndarray, List[str]]:
    """
    åŠ è½½é¢„è®¡ç®—çš„å‘é‡æ•°æ®ï¼ˆå¦‚ SIFT1Mã€GIST1Mã€Ann-benchmarks æ•°æ®é›†ï¼‰
    
    æ”¯æŒçš„æ ¼å¼ï¼š
    1. NumPy æ ¼å¼ (.npy)ï¼šå½¢çŠ¶ä¸º (n, dimension) çš„æ•°ç»„
    2. æ–‡æœ¬æ ¼å¼ (.txt)ï¼šæ¯è¡Œä¸€ä¸ªå‘é‡ï¼Œç©ºæ ¼åˆ†éš”
    
    Args:
        vector_file: å‘é‡æ–‡ä»¶è·¯å¾„
        limit: é™åˆ¶åŠ è½½æ•°é‡
    
    Returns:
        Tuple[np.ndarray, List[str]]: (å‘é‡æ•°ç»„, å…ƒæ•°æ®æ–‡æœ¬åˆ—è¡¨)
    """
    vector_file_path = Path(vector_file)
    
    if not vector_file_path.exists():
        raise FileNotFoundError(f"å‘é‡æ–‡ä»¶ä¸å­˜åœ¨: {vector_file}")
    
    print(f"æ­£åœ¨åŠ è½½é¢„è®¡ç®—å‘é‡: {vector_file}")
    
    if vector_file_path.suffix == '.npy':
        # NumPy æ ¼å¼
        vectors = np.load(vector_file)
        if limit:
            vectors = vectors[:limit]
        
        # ç”Ÿæˆå…ƒæ•°æ®æ–‡æœ¬ï¼ˆå› ä¸ºæ²¡æœ‰åŸå§‹æ–‡æœ¬ï¼‰
        texts = [f"vector_{i+1}" for i in range(len(vectors))]
        print(f"âœ… åŠ è½½äº† {len(vectors)} ä¸ªå‘é‡ï¼ˆç»´åº¦: {vectors.shape[1]}ï¼‰")
        return vectors, texts
    
    elif vector_file_path.suffix in ['.txt', '.dat']:
        # æ–‡æœ¬æ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªå‘é‡ï¼Œç©ºæ ¼æˆ–é€—å·åˆ†éš”ï¼‰
        vectors = []
        texts = []
        with open(vector_file_path, 'r') as f:
            for i, line in enumerate(f):
                if limit and i >= limit:
                    break
                
                parts = line.strip().split()
                if len(parts) > 1:
                    try:
                        vector = [float(x) for x in parts]
                        vectors.append(vector)
                        texts.append(f"vector_{i+1}")
                    except ValueError:
                        continue
        
        if not vectors:
            raise ValueError("æœªèƒ½ä»æ–‡ä»¶ä¸­è§£æå‡ºå‘é‡æ•°æ®")
        
        vectors_array = np.array(vectors)
        print(f"âœ… åŠ è½½äº† {len(vectors_array)} ä¸ªå‘é‡ï¼ˆç»´åº¦: {vectors_array.shape[1]}ï¼‰")
        return vectors_array, texts
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {vector_file_path.suffix}")


def generate_sample_texts(count: int) -> List[str]:
    """
    ç”Ÿæˆç¤ºä¾‹æµ‹è¯•æ–‡æœ¬ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    
    Args:
        count: ç”Ÿæˆæ–‡æœ¬æ•°é‡
    
    Returns:
        List[str]: æ–‡æœ¬åˆ—è¡¨
    """
    # é¢„è®¾çš„æµ‹è¯•æ–‡æœ¬æ¨¡æ¿
    templates = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ è€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯è®¡ç®—æœºç§‘å­¦å’Œäººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºå’Œäººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚",
        "è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿè¯†åˆ«å’Œç†è§£å›¾åƒå’Œè§†é¢‘å†…å®¹ã€‚",
        "æ•°æ®æŒ–æ˜æ˜¯ä»å¤§é‡æ•°æ®ä¸­å‘ç°æ¨¡å¼å’ŒçŸ¥è¯†çš„è¿‡ç¨‹ï¼Œç»“åˆäº†ç»Ÿè®¡å­¦ã€æœºå™¨å­¦ä¹ å’Œæ•°æ®åº“ç³»ç»Ÿã€‚",
        "å¤§æ•°æ®æ˜¯æŒ‡æ•°æ®é‡å¤ªå¤§ã€å˜åŒ–å¤ªå¿«æˆ–ç»“æ„å¤ªå¤æ‚è€Œæ— æ³•ç”¨ä¼ ç»Ÿæ•°æ®å¤„ç†å·¥å…·æœ‰æ•ˆå¤„ç†çš„æ•°æ®é›†ã€‚",
        "äº‘è®¡ç®—æ˜¯é€šè¿‡äº’è”ç½‘æä¾›è®¡ç®—èµ„æºã€å­˜å‚¨å’Œåº”ç”¨ç¨‹åºçš„æœåŠ¡æ¨¡å¼ã€‚",
        "åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œä»¥å®‰å…¨ã€é€æ˜å’Œå»ä¸­å¿ƒåŒ–çš„æ–¹å¼è®°å½•äº¤æ˜“ã€‚",
        "ç‰©è”ç½‘æ˜¯å°†æ—¥å¸¸ç‰©ä½“è¿æ¥åˆ°äº’è”ç½‘ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿæ”¶é›†å’Œäº¤æ¢æ•°æ®çš„æŠ€æœ¯ç½‘ç»œã€‚",
        "å‘é‡æ•°æ®åº“æ˜¯ä¸“é—¨ç”¨äºå­˜å‚¨å’ŒæŸ¥è¯¢é«˜ç»´å‘é‡æ•°æ®çš„æ•°æ®åº“ç³»ç»Ÿï¼Œæ”¯æŒç›¸ä¼¼åº¦æœç´¢ã€‚",
        "è¯­ä¹‰æœç´¢æ˜¯é€šè¿‡ç†è§£æŸ¥è¯¢æ„å›¾å’Œä¸Šä¸‹æ–‡æ¥æ”¹è¿›æœç´¢ç»“æœçš„ä¿¡æ¯æ£€ç´¢æ–¹æ³•ã€‚",
        "æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æ˜¯ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„æŠ€æœ¯ï¼Œç”¨äºæé«˜AIç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚",
        "ç´¢å¼•ä¼˜åŒ–æ˜¯æé«˜æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½çš„å…³é”®æŠ€æœ¯ï¼ŒåŒ…æ‹¬é€‰æ‹©åˆé€‚çš„ç´¢å¼•ç±»å‹å’Œå‚æ•°ã€‚",
        "è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢(ANN)æ˜¯å¿«é€ŸæŸ¥æ‰¾ç›¸ä¼¼å‘é‡çš„ç®—æ³•ï¼Œåœ¨ç²¾åº¦å’Œé€Ÿåº¦ä¹‹é—´å–å¾—å¹³è¡¡ã€‚",
        "HNSWæ˜¯ä¸€ç§åˆ†å±‚å¯¼èˆªå°ä¸–ç•Œå›¾ç®—æ³•ï¼Œç”¨äºé«˜æ•ˆçš„å‘é‡ç›¸ä¼¼åº¦æœç´¢ã€‚",
        "PostgreSQLæ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºå…³ç³»æ•°æ®åº“ç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒæ‰©å±•åŠŸèƒ½å¦‚pgvectorã€‚",
        "å‘é‡åµŒå…¥æ˜¯å°†ç¦»æ•£å¯¹è±¡ï¼ˆå¦‚å•è¯ã€å¥å­æˆ–å›¾åƒï¼‰è½¬æ¢ä¸ºè¿ç»­å‘é‡ç©ºé—´çš„è¡¨ç¤ºæ–¹æ³•ã€‚",
        "ç›¸ä¼¼åº¦åº¦é‡æ˜¯è¡¡é‡ä¸¤ä¸ªå‘é‡ä¹‹é—´ç›¸ä¼¼ç¨‹åº¦çš„æ•°å­¦æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½™å¼¦ç›¸ä¼¼åº¦å’Œæ¬§æ°è·ç¦»ã€‚",
        "æ•°æ®åº“ç´¢å¼•æ˜¯æ•°æ®ç»“æ„ï¼Œç”¨äºå¿«é€Ÿå®šä½å’Œè®¿é—®æ•°æ®åº“ä¸­çš„ç‰¹å®šæ•°æ®ã€‚",
    ]
    
    texts = []
    for i in range(count):
        # ä½¿ç”¨æ¨¡æ¿ï¼Œæ·»åŠ å˜åŒ–
        template = templates[i % len(templates)]
        # å¯ä»¥æ·»åŠ ä¸€äº›å˜åŒ–
        if i > 0:
            text = f"{template} è¿™æ˜¯ç¬¬ {i+1} æ¡æµ‹è¯•æ•°æ®ã€‚"
        else:
            text = template
        texts.append(text)
    
    return texts


def chunk_text(text: str, chunk_size: int = 512, overlap: int = 0) -> List[str]:
    """
    å°†é•¿æ–‡æœ¬åˆ†å—
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        chunk_size: æ¯å—å­—ç¬¦æ•°
        overlap: é‡å å­—ç¬¦æ•°
    
    Returns:
        List[str]: æ–‡æœ¬å—åˆ—è¡¨
    """
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        if chunk.strip():
            chunks.append(chunk)
        start = end - overlap
        if start >= len(text):
            break
    
    return chunks


def create_test_table(client: PgVectorClient, table_name: str, dimension: int, drop_existing: bool = False):
    """
    åˆ›å»ºæµ‹è¯•è¡¨ï¼ˆä¸ generate_test_data.py ä¸€è‡´çš„ç»“æ„ï¼‰
    
    Args:
        client: æ•°æ®åº“å®¢æˆ·ç«¯
        table_name: è¡¨å
        dimension: å‘é‡ç»´åº¦
        drop_existing: æ˜¯å¦åˆ é™¤å·²å­˜åœ¨çš„è¡¨
    """
    if drop_existing:
        drop_query = f"DROP TABLE IF EXISTS {table_name} CASCADE;"
        client.execute_update(drop_query)
        print(f"âœ… å·²åˆ é™¤è¡¨ {table_name}")
    
    create_table_query = f"""
    CREATE TABLE IF NOT EXISTS {table_name} (
        id SERIAL PRIMARY KEY,
        name TEXT NOT NULL,
        description TEXT,
        embedding vector({dimension}),
        metadata JSONB,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """
    client.execute_update(create_table_query)
    print(f"âœ… è¡¨ {table_name} å·²åˆ›å»ºæˆ–å·²å­˜åœ¨")


def insert_text_vectors_batch(client: PgVectorClient, table_name: str, 
                                texts: List[str], vectors: np.ndarray,
                                batch_size: int = 100, verbose: bool = True):
    """
    æ‰¹é‡æ’å…¥æ–‡æœ¬å‘é‡æ•°æ®
    
    Args:
        client: æ•°æ®åº“å®¢æˆ·ç«¯
        table_name: è¡¨å
        texts: æ–‡æœ¬åˆ—è¡¨
        vectors: å‘é‡æ•°ç»„
        batch_size: æ‰¹å¤„ç†å¤§å°
        verbose: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
    """
    count = len(vectors)
    dimension = vectors.shape[1]
    
    if len(texts) != count:
        raise ValueError(f"æ–‡æœ¬æ•°é‡({len(texts)})ä¸å‘é‡æ•°é‡({count})ä¸åŒ¹é…")
    
    # å‡†å¤‡æ’å…¥ SQL
    insert_query = f"""
    INSERT INTO {table_name} (name, description, embedding, metadata)
    VALUES (%s, %s, %s::vector, %s::jsonb)
    """
    
    # æ‰¹é‡æ’å…¥
    total_batches = (count + batch_size - 1) // batch_size
    inserted = 0
    
    if verbose:
        pbar = tqdm(total=count, desc="æ’å…¥å‘é‡æ•°æ®")
    
    for i in range(0, count, batch_size):
        batch_texts = texts[i:i+batch_size]
        batch_vectors = vectors[i:i+batch_size]
        batch_data = []
        
        for j, (text, vector) in enumerate(zip(batch_texts, batch_vectors)):
            # å°†å‘é‡è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
            vector_str = '[' + ','.join(map(str, vector)) + ']'
            name = f"text_item_{i+j+1}"
            description = text[:200] if len(text) > 200 else text  # é™åˆ¶é•¿åº¦
            # å­˜å‚¨å®Œæ•´æ–‡æœ¬åˆ° metadataï¼ˆä½¿ç”¨ JSON å®‰å…¨è½¬ä¹‰ï¼‰
            metadata_dict = {"full_text": text[:1000]}
            metadata_json = json.dumps(metadata_dict, ensure_ascii=False)
            batch_data.append((name, description, vector_str, metadata_json))
        
        # æ‰§è¡Œæ‰¹é‡æ’å…¥
        with client.get_connection() as conn:
            cur = conn.cursor()
            cur.executemany(insert_query, batch_data)
            conn.commit()
        
        inserted += len(batch_data)
        if verbose:
            pbar.update(len(batch_data))
    
    if verbose:
        pbar.close()
    
    print(f"âœ… æˆåŠŸæ’å…¥ {inserted} æ¡æ–‡æœ¬å‘é‡æ•°æ®")


def verify_data(client: PgVectorClient, table_name: str):
    """
    éªŒè¯æ’å…¥çš„æ•°æ®ï¼ˆä¸ generate_test_data.py ä¸€è‡´ï¼‰
    
    Args:
        client: æ•°æ®åº“å®¢æˆ·ç«¯
        table_name: è¡¨å
    """
    # ç»Ÿè®¡æ€»è®°å½•æ•°
    count_query = f"SELECT COUNT(*) as count FROM {table_name};"
    result = client.execute_query(count_query)
    total_count = result[0]['count'] if result else 0
    
    # è·å–å‘é‡ç»´åº¦
    dimension = 0
    try:
        sample_query = f"""
        SELECT 
            array_length(
                string_to_array(
                    trim(both '[]' from embedding::text),
                    ','
                ),
                1
            ) as dim
        FROM {table_name}
        WHERE embedding IS NOT NULL
        LIMIT 1;
        """
        sample_result = client.execute_query(sample_query)
        if sample_result and sample_result[0]['dim']:
            dimension = int(sample_result[0]['dim'])
    except Exception:
        pass
    
    # è·å–è¡¨å¤§å°
    size_query = f"""
    SELECT pg_size_pretty(pg_total_relation_size('{table_name}')) as size;
    """
    size_result = client.execute_query(size_query)
    table_size = size_result[0]['size'] if size_result else 'N/A'
    
    print("\n" + "="*60)
    print("æ•°æ®éªŒè¯ç»“æœ")
    print("="*60)
    print(f"è¡¨å: {table_name}")
    print(f"æ€»è®°å½•æ•°: {total_count:,}")
    print(f"å‘é‡ç»´åº¦: {dimension}")
    print(f"è¡¨å¤§å°: {table_size}")
    print("="*60)


def main():
    parser = argparse.ArgumentParser(description='ç”ŸæˆçœŸå®æ–‡æœ¬å‘é‡æ•°æ®')
    parser.add_argument('--count', type=int, default=None,
                       help='é™åˆ¶å‘é‡æ•°é‡ï¼ˆå¯é€‰ï¼Œç”¨äºé™åˆ¶åŠ è½½çš„æ•°æ®é‡ï¼‰')
    parser.add_argument('--table-name', type=str, default=TEST_TABLE_NAME,
                       help=f'è¡¨åï¼ˆé»˜è®¤ï¼š{TEST_TABLE_NAME}ï¼‰')
    parser.add_argument('--batch-size', type=int, default=100,
                       help='æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤ï¼š100ï¼‰')
    parser.add_argument('--embedding-batch-size', type=int, default=32,
                       help='Embedding æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤ï¼š32ï¼‰')
    
    # æ•°æ®æºé€‰é¡¹
    data_source_group = parser.add_mutually_exclusive_group(required=True)
    data_source_group.add_argument('--sample', action='store_true',
                                   help='ä½¿ç”¨é¢„è®¾ç¤ºä¾‹æ–‡æœ¬')
    data_source_group.add_argument('--file', type=str,
                                   help='ä»æ–‡æœ¬æ–‡ä»¶è¯»å–ï¼ˆæ¯è¡Œä¸€ä¸ªæ–‡æœ¬ï¼‰')
    data_source_group.add_argument('--dir', type=str,
                                   help='ä»ç›®å½•è¯»å–æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶')
    data_source_group.add_argument('--vectors', type=str,
                                   help='ç›´æ¥åŠ è½½é¢„è®¡ç®—å‘é‡æ–‡ä»¶ï¼ˆ.npy æˆ– .txt æ ¼å¼ï¼Œå¦‚ SIFT1Mã€GIST1M æ•°æ®é›†ï¼‰')
    data_source_group.add_argument('--wikipedia', type=str,
                                   help='ä¸‹è½½ Wikipedia æ–‡æœ¬ï¼ˆç¤ºä¾‹å®ç°ï¼Œå®é™…å»ºè®®ä½¿ç”¨å·²å¤„ç†çš„æ–‡ä»¶ï¼‰')
    
    parser.add_argument('--chunk-size', type=int, default=0,
                       help='æ–‡æœ¬åˆ†å—å¤§å°ï¼ˆ0 è¡¨ç¤ºä¸åˆ†å—ï¼Œé»˜è®¤ï¼š0ï¼‰')
    parser.add_argument('--chunk-overlap', type=int, default=0,
                       help='æ–‡æœ¬åˆ†å—é‡å å¤§å°ï¼ˆé»˜è®¤ï¼š0ï¼‰')
    parser.add_argument('--drop-existing', action='store_true',
                       help='åˆ é™¤å·²å­˜åœ¨çš„è¡¨')
    parser.add_argument('--no-verify', action='store_true',
                       help='ä¸éªŒè¯æ’å…¥çš„æ•°æ®')
    
    args = parser.parse_args()
    
    print("="*60)
    print("ç”ŸæˆçœŸå®æ–‡æœ¬å‘é‡æ•°æ®")
    print("="*60)
    
    # ç¡®å®šæ•°æ®æºç±»å‹
    if args.sample:
        data_source_type = 'ç¤ºä¾‹æ–‡æœ¬'
        data_source_desc = f"æ•°é‡: {args.count or 'é»˜è®¤'}"
    elif args.file:
        data_source_type = 'æ–‡æœ¬æ–‡ä»¶'
        data_source_desc = f"æ–‡ä»¶: {args.file}"
    elif args.dir:
        data_source_type = 'ç›®å½•'
        data_source_desc = f"ç›®å½•: {args.dir}"
    elif args.vectors:
        data_source_type = 'é¢„è®¡ç®—å‘é‡'
        data_source_desc = f"æ–‡ä»¶: {args.vectors}"
    elif args.wikipedia:
        data_source_type = 'Wikipediaï¼ˆç¤ºä¾‹ï¼‰'
        data_source_desc = f"è¾“å‡ºæ–‡ä»¶: {args.wikipedia}"
    
    print(f"æ•°æ®æº: {data_source_type}")
    print(f"  {data_source_desc}")
    print(f"è¡¨å: {args.table_name}")
    
    if args.vectors:
        print(f"âš ï¸  æ³¨æ„ï¼šé¢„è®¡ç®—å‘é‡æ–‡ä»¶çš„ç»´åº¦å¿…é¡»ä¸æ¨¡å‹ç»´åº¦({MODEL_DIMENSION})åŒ¹é…ï¼Œå¦åˆ™éœ€è¦è°ƒæ•´")
    else:
        print(f"æ–‡æœ¬åˆ†å—: {'æ˜¯' if args.chunk_size > 0 else 'å¦'}")
        if args.chunk_size > 0:
            print(f"  åˆ†å—å¤§å°: {args.chunk_size}")
            print(f"  é‡å å¤§å°: {args.chunk_overlap}")
    print("="*60)
    
    # åˆ›å»ºæ•°æ®åº“å®¢æˆ·ç«¯
    client = PgVectorClient(**DB_CONFIG)
    
    # æµ‹è¯•è¿æ¥
    if not client.test_connection():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼")
        return
    
    # æ£€æŸ¥æ‰©å±•
    if not client.check_extension('vector'):
        print("âŒ pgvector æ‰©å±•æœªå®‰è£…ï¼")
        print("è¯·åœ¨æ•°æ®åº“ä¸­æ‰§è¡Œ: CREATE EXTENSION IF NOT EXISTS vector;")
        return
    
    # å¤„ç†é¢„è®¡ç®—å‘é‡æ•°æ®ï¼ˆç›´æ¥åŠ è½½ï¼Œä¸éœ€è¦ embeddingï¼‰
    if args.vectors:
        vectors, texts = load_precomputed_vectors(args.vectors, args.count)
        actual_dimension = vectors.shape[1]
        
        # éªŒè¯ç»´åº¦
        if actual_dimension != MODEL_DIMENSION:
            print(f"âš ï¸  è­¦å‘Šï¼šå‘é‡ç»´åº¦({actual_dimension})ä¸é…ç½®çš„æ¨¡å‹ç»´åº¦({MODEL_DIMENSION})ä¸ä¸€è‡´")
            response = input(f"æ˜¯å¦ç»§ç»­ä½¿ç”¨ç»´åº¦ {actual_dimension}ï¼Ÿ(y/n): ")
            if response.lower() != 'y':
                print("âŒ å·²å–æ¶ˆæ“ä½œ")
                return
            actual_dimension = vectors.shape[1]
        
        print(f"\nâœ… å·²åŠ è½½é¢„è®¡ç®—å‘é‡ï¼ˆè·³è¿‡ embedding æ­¥éª¤ï¼‰")
        print(f"  å‘é‡æ•°é‡: {len(vectors)}")
        print(f"  å‘é‡ç»´åº¦: {actual_dimension}")
    
    else:
        # æ–‡æœ¬æ•°æ®éœ€è¦ embedding
        # åˆå§‹åŒ–æ–‡æœ¬å‘é‡åŒ–å·¥å…·
        print("\nåˆå§‹åŒ–æ–‡æœ¬å‘é‡åŒ–å·¥å…·...")
        embedder = TextEmbedder(use_local_only=True)
        embedder.load()
        actual_dimension = embedder.get_dimension()
        
        # è¯»å–æ–‡æœ¬æ•°æ®
        print("\nè¯»å–æ–‡æœ¬æ•°æ®...")
        texts = []
        
        if args.sample:
            count = args.count if args.count else 1000  # é»˜è®¤ 1000
            texts = generate_sample_texts(count)
            print(f"âœ… ç”Ÿæˆäº† {len(texts)} æ¡ç¤ºä¾‹æ–‡æœ¬")
        elif args.file:
            texts = read_text_file(args.file)
            print(f"âœ… ä»æ–‡ä»¶è¯»å–äº† {len(texts)} æ¡æ–‡æœ¬")
        elif args.dir:
            all_texts = []
            for file_path, content in read_text_directory(args.dir):
                if args.chunk_size > 0:
                    chunks = chunk_text(content, args.chunk_size, args.chunk_overlap)
                    all_texts.extend(chunks)
                else:
                    all_texts.append(content)
            texts = all_texts
            print(f"âœ… ä»ç›®å½•è¯»å–äº† {len(texts)} æ¡æ–‡æœ¬ï¼ˆæˆ–æ–‡æœ¬å—ï¼‰")
        elif args.wikipedia:
            # Wikipedia ç¤ºä¾‹ï¼ˆå®é™…ä½¿ç”¨å»ºè®®ä¸‹è½½åä½¿ç”¨ --file æˆ– --dirï¼‰
            wikipedia_file = download_wikipedia_sample(args.wikipedia, args.count)
            texts = read_text_file(wikipedia_file)
            print(f"âœ… ä» Wikipedia ç¤ºä¾‹æ–‡ä»¶è¯»å–äº† {len(texts)} æ¡æ–‡æœ¬")
        
        if not texts:
            print("âŒ æœªè¯»å–åˆ°ä»»ä½•æ–‡æœ¬æ•°æ®ï¼")
            return
        
        # é™åˆ¶æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.count and len(texts) > args.count:
            texts = texts[:args.count]
            print(f"ğŸ“ é™åˆ¶ä¸º {len(texts)} æ¡æ–‡æœ¬")
        
        # ç”Ÿæˆå‘é‡
        print(f"\nç”Ÿæˆå‘é‡ï¼ˆä½¿ç”¨æ¨¡å‹: {embedder.model_name}ï¼‰...")
        print(f"æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"å‘é‡ç»´åº¦: {actual_dimension}")
        
        vectors = embedder.encode(
            texts,
            batch_size=args.embedding_batch_size,
            show_progress_bar=True
        )
        
        print(f"âœ… ç”Ÿæˆäº† {len(vectors)} ä¸ªå‘é‡")
    
    # åˆ›å»ºè¡¨
    print("\nåˆ›å»ºæµ‹è¯•è¡¨...")
    create_test_table(client, args.table_name, actual_dimension, args.drop_existing)
    
    # æ’å…¥æ•°æ®
    print("\næ’å…¥å‘é‡æ•°æ®åˆ°æ•°æ®åº“...")
    insert_text_vectors_batch(
        client, args.table_name, texts, vectors,
        args.batch_size, verbose=True
    )
    
    # éªŒè¯æ•°æ®
    if not args.no_verify:
        verify_data(client, args.table_name)
    
    print("\nâœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print("\nğŸ’¡ æç¤ºï¼š")
    print("   - ç°åœ¨å¯ä»¥ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•ç´¢å¼•æ€§èƒ½")
    print("   - ä½¿ç”¨ performance_testing/test_hnsw_index.py æµ‹è¯•ç´¢å¼•")
    print("   - ä½¿ç”¨ performance_testing/query_accuracy.py è¯„ä¼°æŸ¥è¯¢ç²¾åº¦")


if __name__ == "__main__":
    main()

