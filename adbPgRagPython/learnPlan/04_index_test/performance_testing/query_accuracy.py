"""
æŸ¥è¯¢ç²¾åº¦è¯„ä¼°å·¥å…·
ç”¨äºè¯„ä¼°ç´¢å¼•æŸ¥è¯¢çš„å¬å›ç‡ï¼ˆRecall@Kï¼‰å’Œå‡†ç¡®ç‡
éœ€è¦çœŸå®æ•°æ®æ‰èƒ½è¿›è¡Œæœ‰æ„ä¹‰çš„ç²¾åº¦è¯„ä¼°
"""

import sys
import argparse
import time
import importlib.util
from pathlib import Path
from typing import List, Set, Dict, Tuple
import numpy as np
from tqdm import tqdm

# æ˜ç¡®å¯¼å…¥ 04_index_test çš„ configï¼ˆé¿å…ä¸ 05_simple_test çš„ config å†²çªï¼‰
_04_index_test_path = Path(__file__).parent.parent
_config_path = _04_index_test_path / 'config.py'
spec = importlib.util.spec_from_file_location("_config_04", _config_path)
_config_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(_config_module)
DB_CONFIG = _config_module.DB_CONFIG
TEST_TABLE_NAME = _config_module.TEST_TABLE_NAME

# æ·»åŠ  05_simple_test è·¯å¾„ï¼ˆç”¨äºå¯¼å…¥ embedding_utils å’Œ db_utilsï¼‰
_05_simple_test_path = Path(__file__).parent.parent.parent / '05_simple_test'
sys.path.insert(0, str(_05_simple_test_path))

from db_utils import PgVectorClient
from embedding_utils import TextEmbedder


def get_true_nearest_neighbors(
    client: PgVectorClient,
    table_name: str,
    query_vector: np.ndarray,
    k: int,
    distance_op: str = '<=>'
) -> List[int]:
    """
    ä½¿ç”¨æš´åŠ›æœç´¢è·å–çœŸå®æœ€è¿‘é‚»ï¼ˆä½œä¸ºåŸºå‡†ï¼‰
    
    Args:
        client: æ•°æ®åº“å®¢æˆ·ç«¯
        table_name: è¡¨å
        query_vector: æŸ¥è¯¢å‘é‡
        k: è¿”å› Top-K ç»“æœ
        distance_op: è·ç¦»æ“ä½œç¬¦ï¼ˆ<=> ä½™å¼¦è·ç¦»ï¼Œ<-> æ¬§æ°è·ç¦»ï¼‰
    
    Returns:
        List[int]: çœŸå®æœ€è¿‘é‚»çš„ id åˆ—è¡¨ï¼ˆæŒ‰è·ç¦»å‡åºï¼‰
    """
    vector_str = '[' + ','.join(map(str, query_vector)) + ']'
    
    # ä½¿ç”¨æš´åŠ›æœç´¢ï¼ˆä¸ä½¿ç”¨ç´¢å¼•ï¼‰
    query = f"""
    SELECT id, embedding {distance_op} %s::vector as distance
    FROM {table_name}
    ORDER BY embedding {distance_op} %s::vector
    LIMIT {k};
    """
    
    # å¼ºåˆ¶ä¸ä½¿ç”¨ç´¢å¼•
    client.execute_update("SET enable_seqscan = on;")
    client.execute_update("SET enable_indexscan = off;")
    
    result = client.execute_query(query, (vector_str, vector_str))
    
    # æ¢å¤ç´¢å¼•è®¾ç½®
    client.execute_update("SET enable_seqscan = on;")
    client.execute_update("SET enable_indexscan = on;")
    
    if result:
        return [row['id'] for row in result]
    return []


def get_index_query_results(
    client: PgVectorClient,
    table_name: str,
    query_vector: np.ndarray,
    k: int,
    ef_search: int = None,
    distance_op: str = '<=>'
) -> List[int]:
    """
    ä½¿ç”¨ç´¢å¼•æŸ¥è¯¢è·å–ç»“æœ
    
    Args:
        client: æ•°æ®åº“å®¢æˆ·ç«¯
        table_name: è¡¨å
        query_vector: æŸ¥è¯¢å‘é‡
        k: è¿”å› Top-K ç»“æœ
        ef_search: HNSW ef_search å‚æ•°ï¼ˆä»…å¯¹ HNSW ç´¢å¼•æœ‰æ•ˆï¼‰
        distance_op: è·ç¦»æ“ä½œç¬¦
    
    Returns:
        List[int]: æŸ¥è¯¢ç»“æœçš„ id åˆ—è¡¨ï¼ˆæŒ‰è·ç¦»å‡åºï¼‰
    """
    vector_str = '[' + ','.join(map(str, query_vector)) + ']'
    
    # è®¾ç½®æŸ¥è¯¢å‚æ•°
    if ef_search:
        client.execute_update(f"SET hnsw.ef_search = {ef_search};")
    
    query = f"""
    SELECT id, embedding {distance_op} %s::vector as distance
    FROM {table_name}
    ORDER BY embedding {distance_op} %s::vector
    LIMIT {k};
    """
    
    result = client.execute_query(query, (vector_str, vector_str))
    
    if result:
        return [row['id'] for row in result]
    return []


def calculate_recall_at_k(
    true_neighbors: List[int],
    query_results: List[int],
    k: int
) -> float:
    """
    è®¡ç®—å¬å›ç‡@Kï¼ˆRecall@Kï¼‰
    
    Recall@K = |æŸ¥è¯¢ç»“æœ âˆ© çœŸå®æœ€è¿‘é‚»| / min(K, |çœŸå®æœ€è¿‘é‚»|)
    
    Args:
        true_neighbors: çœŸå®æœ€è¿‘é‚» id åˆ—è¡¨
        query_results: æŸ¥è¯¢ç»“æœ id åˆ—è¡¨
        k: K å€¼
    
    Returns:
        float: å¬å›ç‡ï¼ˆ0.0 - 1.0ï¼‰
    """
    if not true_neighbors:
        return 0.0
    
    # å–å‰ k ä¸ªçœŸå®æœ€è¿‘é‚»
    true_set = set(true_neighbors[:k])
    
    # å–å‰ k ä¸ªæŸ¥è¯¢ç»“æœ
    result_set = set(query_results[:k])
    
    # è®¡ç®—äº¤é›†
    intersection = true_set & result_set
    
    # å¬å›ç‡ = äº¤é›†å¤§å° / min(k, çœŸå®æœ€è¿‘é‚»æ•°é‡)
    denominator = min(k, len(true_neighbors))
    
    if denominator == 0:
        return 0.0
    
    recall = len(intersection) / denominator
    return recall


def calculate_precision_at_k(
    true_neighbors: List[int],
    query_results: List[int],
    k: int
) -> float:
    """
    è®¡ç®—ç²¾ç¡®ç‡@Kï¼ˆPrecision@Kï¼‰
    
    Precision@K = |æŸ¥è¯¢ç»“æœ âˆ© çœŸå®æœ€è¿‘é‚»| / min(K, |æŸ¥è¯¢ç»“æœ|)
    
    Args:
        true_neighbors: çœŸå®æœ€è¿‘é‚» id åˆ—è¡¨
        query_results: æŸ¥è¯¢ç»“æœ id åˆ—è¡¨
        k: K å€¼
    
    Returns:
        float: ç²¾ç¡®ç‡ï¼ˆ0.0 - 1.0ï¼‰
    """
    if not query_results:
        return 0.0
    
    true_set = set(true_neighbors)
    result_set = set(query_results[:k])
    
    intersection = result_set & true_set
    
    denominator = min(k, len(query_results))
    
    if denominator == 0:
        return 0.0
    
    precision = len(intersection) / denominator
    return precision


def evaluate_query_accuracy(
    client: PgVectorClient,
    embedder: TextEmbedder,
    table_name: str,
    query_texts: List[str],
    k: int = 10,
    ef_search: int = 40,
    distance_op: str = '<=>',
    num_queries: int = None,
    verbose: bool = True
) -> Dict:
    """
    è¯„ä¼°æŸ¥è¯¢ç²¾åº¦
    
    Args:
        client: æ•°æ®åº“å®¢æˆ·ç«¯
        embedder: æ–‡æœ¬å‘é‡åŒ–å·¥å…·
        table_name: è¡¨å
        query_texts: æŸ¥è¯¢æ–‡æœ¬åˆ—è¡¨
        k: Top-K ç»“æœæ•°
        ef_search: HNSW ef_search å‚æ•°
        distance_op: è·ç¦»æ“ä½œç¬¦
        num_queries: è¯„ä¼°çš„æŸ¥è¯¢æ•°é‡ï¼ˆNone è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æŸ¥è¯¢æ–‡æœ¬ï¼‰
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
    
    Returns:
        Dict: è¯„ä¼°ç»“æœç»Ÿè®¡
    """
    if num_queries:
        query_texts = query_texts[:num_queries]
    
    print(f"\nè¯„ä¼°æŸ¥è¯¢ç²¾åº¦")
    print(f"  æŸ¥è¯¢æ•°é‡: {len(query_texts)}")
    print(f"  K: {k}")
    print(f"  ef_search: {ef_search}")
    print(f"  è·ç¦»æ“ä½œç¬¦: {distance_op}")
    
    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    if verbose:
        print("\nç”ŸæˆæŸ¥è¯¢å‘é‡...")
    query_vectors = embedder.encode(query_texts, show_progress_bar=verbose)
    
    # è¯„ä¼°æ¯ä¸ªæŸ¥è¯¢
    recalls = []
    precisions = []
    query_times = []
    
    if verbose:
        pbar = tqdm(total=len(query_texts), desc="è¯„ä¼°æŸ¥è¯¢ç²¾åº¦")
    
    for i, (query_text, query_vector) in enumerate(zip(query_texts, query_vectors)):
        # è·å–çœŸå®æœ€è¿‘é‚»ï¼ˆæš´åŠ›æœç´¢ï¼‰
        start_time = time.time()
        true_neighbors = get_true_nearest_neighbors(
            client, table_name, query_vector, k, distance_op
        )
        true_search_time = time.time() - start_time
        
        # ä½¿ç”¨ç´¢å¼•æŸ¥è¯¢
        start_time = time.time()
        query_results = get_index_query_results(
            client, table_name, query_vector, k, ef_search, distance_op
        )
        index_query_time = time.time() - start_time
        query_times.append(index_query_time * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # è®¡ç®—å¬å›ç‡å’Œç²¾ç¡®ç‡
        recall = calculate_recall_at_k(true_neighbors, query_results, k)
        precision = calculate_precision_at_k(true_neighbors, query_results, k)
        
        recalls.append(recall)
        precisions.append(precision)
        
        if verbose:
            pbar.update(1)
    
    if verbose:
        pbar.close()
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    recalls = np.array(recalls)
    precisions = np.array(precisions)
    query_times = np.array(query_times)
    
    stats = {
        'num_queries': len(query_texts),
        'k': k,
        'ef_search': ef_search,
        'avg_recall': float(np.mean(recalls)),
        'median_recall': float(np.median(recalls)),
        'min_recall': float(np.min(recalls)),
        'max_recall': float(np.max(recalls)),
        'avg_precision': float(np.mean(precisions)),
        'median_precision': float(np.median(precisions)),
        'avg_query_time_ms': float(np.mean(query_times)),
        'median_query_time_ms': float(np.median(query_times)),
        'p95_query_time_ms': float(np.percentile(query_times, 95)),
    }
    
    return stats


def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°æŸ¥è¯¢ç²¾åº¦ï¼ˆå¬å›ç‡ï¼‰')
    parser.add_argument('--table-name', type=str, default=TEST_TABLE_NAME,
                       help=f'è¡¨åï¼ˆé»˜è®¤ï¼š{TEST_TABLE_NAME}ï¼‰')
    parser.add_argument('--k', type=int, default=10,
                       help='Top-K ç»“æœæ•°ï¼ˆé»˜è®¤ï¼š10ï¼‰')
    parser.add_argument('--ef-search', type=int, default=40,
                       help='HNSW ef_search å‚æ•°ï¼ˆé»˜è®¤ï¼š40ï¼‰')
    parser.add_argument('--num-queries', type=int, default=50,
                       help='è¯„ä¼°æŸ¥è¯¢æ•°é‡ï¼ˆé»˜è®¤ï¼š50ï¼‰')
    parser.add_argument('--distance-op', type=str, default='<=>',
                       choices=['<=>', '<->', '<#>'],
                       help='è·ç¦»æ“ä½œç¬¦ï¼ˆé»˜è®¤ï¼š<=> ä½™å¼¦è·ç¦»ï¼‰')
    parser.add_argument('--query-file', type=str,
                       help='æŸ¥è¯¢æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä»è¡¨ä¸­éšæœºé€‰æ‹©ï¼‰')
    parser.add_argument('--sample-queries', action='store_true',
                       help='ä»è¡¨ä¸­éšæœºé‡‡æ ·æŸ¥è¯¢ï¼ˆä¸ä½¿ç”¨æ–‡ä»¶ï¼‰')
    
    args = parser.parse_args()
    
    print("="*60)
    print("æŸ¥è¯¢ç²¾åº¦è¯„ä¼°")
    print("="*60)
    print(f"è¡¨å: {args.table_name}")
    print(f"K: {args.k}")
    print(f"ef_search: {args.ef_search}")
    print(f"æŸ¥è¯¢æ•°é‡: {args.num_queries}")
    print("="*60)
    
    # åˆ›å»ºæ•°æ®åº“å®¢æˆ·ç«¯
    client = PgVectorClient(**DB_CONFIG)
    
    # æµ‹è¯•è¿æ¥
    if not client.test_connection():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼")
        return
    
    # åˆå§‹åŒ–æ–‡æœ¬å‘é‡åŒ–å·¥å…·
    print("\nåˆå§‹åŒ–æ–‡æœ¬å‘é‡åŒ–å·¥å…·...")
    embedder = TextEmbedder(use_local_only=True)
    embedder.load()
    
    # å‡†å¤‡æŸ¥è¯¢æ–‡æœ¬
    query_texts = []
    
    if args.query_file:
        # ä»æ–‡ä»¶è¯»å–æŸ¥è¯¢
        with open(args.query_file, 'r', encoding='utf-8') as f:
            query_texts = [line.strip() for line in f if line.strip()]
        print(f"âœ… ä»æ–‡ä»¶è¯»å–äº† {len(query_texts)} ä¸ªæŸ¥è¯¢")
    elif args.sample_queries:
        # ä»è¡¨ä¸­éšæœºé‡‡æ ·
        query = f"""
        SELECT description FROM {args.table_name}
        ORDER BY RANDOM()
        LIMIT {args.num_queries};
        """
        results = client.execute_query(query)
        query_texts = [row['description'] for row in results if row['description']]
        print(f"âœ… ä»è¡¨ä¸­éšæœºé‡‡æ ·äº† {len(query_texts)} ä¸ªæŸ¥è¯¢")
    else:
        # ä½¿ç”¨é¢„è®¾ç¤ºä¾‹æŸ¥è¯¢
        sample_queries = [
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "æœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Ÿ",
            "å‘é‡æ•°æ®åº“çš„åº”ç”¨åœºæ™¯æœ‰å“ªäº›ï¼Ÿ",
            "HNSW ç®—æ³•çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
        ]
        query_texts = sample_queries * (args.num_queries // len(sample_queries) + 1)
        query_texts = query_texts[:args.num_queries]
        print(f"âœ… ä½¿ç”¨é¢„è®¾ç¤ºä¾‹æŸ¥è¯¢ {len(query_texts)} ä¸ª")
    
    if not query_texts:
        print("âŒ æœªå‡†å¤‡åˆ°ä»»ä½•æŸ¥è¯¢æ–‡æœ¬ï¼")
        return
    
    # è¯„ä¼°ç²¾åº¦
    stats = evaluate_query_accuracy(
        client, embedder, args.table_name,
        query_texts, args.k, args.ef_search,
        args.distance_op, args.num_queries,
        verbose=True
    )
    
    # è¾“å‡ºç»“æœ
    print("\n" + "="*60)
    print("è¯„ä¼°ç»“æœ")
    print("="*60)
    print(f"æŸ¥è¯¢æ•°é‡: {stats['num_queries']}")
    print(f"\nå¬å›ç‡@K (Recall@{stats['k']}):")
    print(f"  å¹³å‡: {stats['avg_recall']:.4f} ({stats['avg_recall']*100:.2f}%)")
    print(f"  ä¸­ä½æ•°: {stats['median_recall']:.4f} ({stats['median_recall']*100:.2f}%)")
    print(f"  æœ€å°å€¼: {stats['min_recall']:.4f}")
    print(f"  æœ€å¤§å€¼: {stats['max_recall']:.4f}")
    
    print(f"\nç²¾ç¡®ç‡@K (Precision@{stats['k']}):")
    print(f"  å¹³å‡: {stats['avg_precision']:.4f} ({stats['avg_precision']*100:.2f}%)")
    print(f"  ä¸­ä½æ•°: {stats['median_precision']:.4f} ({stats['median_precision']*100:.2f}%)")
    
    print(f"\næŸ¥è¯¢æ€§èƒ½:")
    print(f"  å¹³å‡æŸ¥è¯¢æ—¶é—´: {stats['avg_query_time_ms']:.2f} ms")
    print(f"  ä¸­ä½æ•°æŸ¥è¯¢æ—¶é—´: {stats['median_query_time_ms']:.2f} ms")
    print(f"  P95 æŸ¥è¯¢æ—¶é—´: {stats['p95_query_time_ms']:.2f} ms")
    print("="*60)
    
    print("\nğŸ’¡ æç¤º:")
    print("  - å¬å›ç‡è¶Šé«˜ï¼Œè¯´æ˜ç´¢å¼•æ‰¾åˆ°çš„çœŸå®æœ€è¿‘é‚»è¶Šå¤š")
    print("  - å¦‚æœå¬å›ç‡è¾ƒä½ï¼Œå¯ä»¥å°è¯•å¢å¤§ ef_search å‚æ•°")
    print("  - å¬å›ç‡å’ŒæŸ¥è¯¢é€Ÿåº¦éœ€è¦å¹³è¡¡ï¼šef_search è¶Šå¤§ï¼Œå¬å›ç‡è¶Šé«˜ä½†æŸ¥è¯¢è¶Šæ…¢")


if __name__ == "__main__":
    main()

